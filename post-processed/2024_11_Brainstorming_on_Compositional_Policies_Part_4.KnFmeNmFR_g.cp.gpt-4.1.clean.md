I wanted to go over what we discussed over the last month. It was a good review for me, and I want to give a quick recap. I have a whiteboarding example problem to show you.

Starting November 6th, before this, Niels was talking about our various motor policies, and then we moved into discussing goal-oriented behavior. Jeff explained how the cortex creates goal-oriented behaviors, a major missing component of TBT, and introduced a model where the upper layers of the cortex represent the state of an object, and the lower layers represent the space of an object. The idea is that once we have states defined as part of these models, we can start talking about goals and goal states. A goal is defined as the desired state, the state we would like that column to be in.

From the document and conversation on the 6th, there were a few open questions. We have a state space, presumably more abstract than the physical 3D location space. There are questions about what movement looks like and path integration in state space. There will be some connection between the states and the actual representation and the location or space layers, but we don't really know how that works.

On a different level, another theme is whether there is voting and temporal pooling of behavioral states, and how columns work together to generate goal-oriented behavior. This leads to a hierarchical arrangement of columns to figure out how goals and actions propagate. In Slack, Jeff posted some follow-up thoughts. For processing goal-oriented behaviors, we need to include a "where" pathway, meaning we're not just looking at the "what" pathway—objects in their own reference frames—but also considering objects in body space and world coordinates. There was some back and forth about this. Viviane and Niels brought up the idea that we're estimating pose, which is also represented in Monty. The degree to which we need to include this has been discussed.

There's also the need for episodic memory. You have to remember your top-level goal and your second-level goal to execute all the subgoals. This is interesting because I've been thinking about the persistence of object models and states as we move through tasks. For example, if we have a cortical column that's foveal and we start moving around and looking at different things, how do we maintain persistence in our representations as we continue sensing and moving through the world?

We also started talking more explicitly about hierarchy. This is an interesting problem. You can formulate the hierarchy so that you need n levels to have n nested compositions, or there may be an alternate way to do that with fewer levels, as Jeff discussed.

When I say layers, I mean tiers in the hierarchy.

Niels also presented that day and brought up an example from previous discussions. He described a problem that is intriguing.

There's a goal: we wake up tired, and some top-level LM receives a goal state. In this example, the goal state comes from the hypothalamus, saying we want to be less tired. The top-level LM receives the goal state "get less tired" and delegates subtasks down the hierarchy. Through L6 to L5 connections, we move to the kitchen modeling LM, which receives the goal state "get coffee made." That can be broken down further to a coffee machine LM, assuming we've found it and are looking at it, with the goal "get the coffee machine on." That can be broken down further to "get the coffee button on." An important part of this model is that you can always delegate to subcortical control at any point. For example, if the goal is to have the coffee machine on, you need to find the coffee machine first, so you may yield control to subcortical machinery to look around the room, find the coffee machine, and then reestablish the process of moving down the cortical hierarchy to achieve these goals.

Some interesting conversations came out of this, especially about how the recruitment of lower-level islands happens.

If you want to move down the hierarchy—Scott, can I ask a question? Sure. Is the goal just to review, or do we want to discuss these items? Do you want to go through the whole thing? I have a lot of thoughts already. I have just a couple more slides, and then a whiteboard with a new problem. If you want to jump in and discuss—a new problem that we would apply these questions to, or a new way of looking at all this? That's the idea. Is it a new way of looking at these things, or a new problem? It's a new problem that is simplified. After talking about the typing stuff, I boiled it down to what seems like a simpler problem.

Because I think some of the things I said were very speculative, I want to make that clear. Some of the things we just presented here are not correct. We can debate these points, but I haven't said anything yet because you've just been reviewing what we talked about, which was brainstorming. I'll let you finish your review, but these points shouldn't stand as definitive; they were just brainstorming ideas. This is just the story of our conversations up until this point. Most of what we've talked about so far—a good portion—will be wrong, so everyone should bear in mind that's the nature of this. Jump in at any time. I can go back to a slide if there's anything you want to discuss. If you're just reviewing what we talked about, that's great, but I don't want to take it as, "Okay, here we are, let's build on this." That wouldn't be correct. For things I presented that were wrong or that I don't think are right—like, I don't think subcortical processes can go find the coffee cup. That's not subcortical, that's cortex. We can clarify that. For example, I wasn't suggesting that something like that, that's kind of map-based, is subcortical. I think it's more like the goal states, as long as they're primitive enough for a subcortical process, like just going to a particular location or something like that. That's more like walking. Those are pretty limited and mostly genetic—not completely, but it's not like there's a map of the kitchen. I think that's what you meant, Scott. Subcortex doesn't know what the puzzle looks like, and almost all visual recognition is happening in the cortex, with very few exceptions. The part where I talked about modeling goals in the upper cortex layers is still very intriguing, but I consider it extremely speculative. I haven't eliminated it, but I certainly wouldn't say it's established. Let's keep going. I'll say a few things as we go along. 

It's very clear to me that you can't represent all the levels of a hierarchy, whether you're modeling hierarchical composition of objects or compositional behaviors, using the levels in the cortex. You can't say, "I have five subtasks, so I'll use five levels," or "I have a nested structure of six compositional levels, so I'll use six cortex levels." That's almost certainly not true. We know that many other animals do sophisticated things without as many levels as we have, and the levels are not always so clear. All of this suggests that much of what we view as hierarchical is always using the same levels repeatedly, just moving the window up and down the stack of composition. I'm pretty certain that's happening.

It doesn't make sense to me that we would have five layers just to handle objects with five levels of composition. There's got to be another way to solve that.

The only other way I can think of is to have the same set of layers moving up and down over the conceptual stack. You can look at some portion of it at any point in time, which requires some way of remembering where you are, because you have to shift those things around.

If you have a complex, multi-nested object—one big object with two or three subcomponents, each of which can be broken down into several other subcomponents—would you expect that entire object to be represented in a column? No, it can't. You'd need at least two levels in the hierarchy to represent the relationship between two objects. If I have four levels, A, B, C, D, those two levels can represent A and B, B and C, or C and D.

For example, with the Numenta mug, at one point you might be attending to the logo on the coffee cup. Those are your two levels, controlled to some degree by attention, representing those two levels. If you wanted to focus on the Numenta logo, you could represent the letters composing the logo and their relation to the logo itself. At that point, you're not as consciously aware of the mug or the room. There are all these different levels—you could look at the mug on the table or the table in the room—but at any point, you tend to be attending to two of those. You're not really thinking about the other two, but it's misleading because you have a memory of where you were. It feels like you're thinking about the cup because that's what you were thinking about, but at the moment you're attending to the letters, you're not actually thinking about the cup. You can catch this pretty clearly. There was a tool people used for presentations—I forget the name—it was a faddish tool, but maybe people are still using it. You could keep deeper and deeper nesting in things.

That sounds familiar. Anyway, the point was, you can do that, and when I first saw that, I thought it was interesting. There's unlimited depth here, but you can't think of all the depths at once. You can mentally work your way back up and down, but it's not all present at once, and you can only handle certain levels of nesting complexity.

I think this shows you need at least two levels and memory to know where you are, so you can move up and down the stack as needed. That's almost a certainty. I don't think I really caught that the first time in these meetings.

One thing I thought was worth mentioning, since you brought up the hippocampus or some sort of memory at a higher level: we've probably talked about this before, but another thing to consider is the prefrontal cortex, which seems to be where short-term memory is—maybe another type of learning module with a very short time frame for its representations. I'd like to hear some research on that, since my knowledge is probably outdated. I always associate short-term memory with the hippocampal complex.

If it's really happening in the prefrontal cortex, I'd like to know, and I'm not doubting it, I just haven't read many papers on prefrontal cortex. The only theories we have for real fast short-term memory—meaning you can remember something quickly and then forget it again—are the silent synapses. I don't even know if they exist in the hippocampus, but they may also exist in prefrontal cortex. That would be interesting to know. My understanding is there's pretty good evidence that a lot of short-term memory involves the prefrontal cortex. My understanding was the bigger debate is whether that's mediated just through persistent activations or through more permanent synaptic changes. People talk about persistent activation, but there are only very special cases for it, like when you're asked to memorize a set of digits—most people can do seven—and you have to keep practicing in your head. Some very short-term memory requires reactivation, but anything where you can get distracted and come back later, I don't believe is persistent activation. I'd like to see the evidence for it, but every time people speculate about it, I haven't seen it. In my view, it almost has to involve some sort of physical modification. 

Some recent theories, as often happens in neuroscience when there are two opposing views, suggest it's a mixture of both. There's some element of persistent activity and some element of synaptic changes, which fits with the finding that if you get distracted while holding something in short-term memory, you often lose that information, but not always. So it's probably something in between. I read some theoretical analysis about persistent activity once, and it's really hard for neurons to maintain it. They operate on the order of milliseconds and want to move on to something else. That would be interesting to research, because I've kind of written off the persistent activity theory as useful for our work, but I could be wrong, so that would be interesting to know. 

If persistent activity was a real way of storing short-term memories, you could theoretically do it in any learning module. You could have some sort of persistence in any learning module. I have no evidence for that. All the evidence I've seen is not. If it's going to be synaptic changes, you can't grow synapses fast enough—it could take at least a minute. Therefore, you need some other way, and these silent synapses fit the bill. You have neurons in the hippocampus—not 3,000 synapses, but 30,000—and most are silent, so you can quickly enable or disable a silent synapse, which is just a chemical process.

That's how I've been thinking about it, but I could be wrong. It would be important to know if other learning areas of the brain, like prefrontal cortex or other modules, could do short-term memory beyond just memorizing seven digits. 

Anyway, sorry, I didn't mean to derail. I guess I was just—no, I derailed it—trying to clarify. A lot of people ask about the hippocampus and, in general, how we keep track of variables in the environment that are quickly changing. It's useful to think that most of these things are probably still learning modules, just with a shorter time scale, whether episodic or short-term memory. There are parallels between the hippocampus, lower levels, and upper levels of the cortex, and people speculate they're very similar. The way I've reviewed it, the hippocampus is different, but it's also the predecessor of cortex and has the same basic features. So I've thought that at the top of our hierarchy, we'll have some learning modules that are fast memory. They're going to have grid cells, place cells, the same kind of things we have in learning modules.

We want to simplify if we can, and that's a good simplification. If anyone sees any papers about prefrontal cortex doing short-term memory, I'd love to read those.

Yeah, I'll try to find the two or three I'm thinking of and share those.

That'd be great.

Scott, go on. Okay, this is just brief, but in the next couple of weeks, we have Jeff first. He's really emphasizing how we can use a couple of columns hierarchically connected and represent composition that way. Then we turn to this problem of typing.

In that discussion about typing, we talked about whether we have a keyboard model, or if we had one and don't anymore, because we're unable to just type. We don't necessarily even know where the keys are or can't pull that out, which is a side point, but I won't derail things too much. There was a lot of back and forth about model-free and model-based approaches, subcortical control, and it got fairly complicated. There were a couple of points I got mentally stuck on, and I wanted to work them out in a more concrete way. One of these problems is if you have a goal state, how do you select the next LM down in the hierarchy?

Part of that is learning causal relationships. When you say, how do you select the next element down the hierarchy, one of the things we discussed was that you could type with any one of your fingers, or do it with your nose. Is that what you mean by this question? Or, in typing, you could grab a chopstick and press the key, or press it with something else—there are all these options. There's some complex process. Someone suggested earlier that we just do the one that's most convenient for us. We would typically poke things with our index fingers, and we'd typically use the one that's nearest to what we want to do. That's a pretty simple policy, and the task of such typing is learning not to use your index fingers. That's the flip. Some of that might be easy, but sometimes if you're presented with a really difficult problem, you don't even know how to solve it. What is the next step you take? It's not as simple as, how do I push this button? For example, my hot water's not working in my house right now. What would be the next step I would take to figure that out? It's not working correctly; it's working.

So it's not—the finger one is pretty simple. Use your index finger, and if you have a choice between your left and right hand, pick your favorite hand, or the one that's closest. That's a simple step for how you actuate this, but if it's a more difficult goal step, it's not so clear how you choose.

I just had another thought about where the keyboard model is learned and whether it's forgotten. One possibility for why the keyboard is such a strange example—where we can't really remember where the letters are, yet we can type them very quickly—might be that the keyboard is actually an object with too many details to be remembered in one model. It's too complex to model as a whole. We don't actually store the entire model of the keyboard, maybe just an approximation of where the numbers are, where the letters are, where the escape key is, and the space bar. It's just too complex to model the whole keyboard. Also, when we type, we don't have time to use a model-based policy. We want to type much quicker than we could with a model-based policy, so we quickly learn these fast motor sequences for words as more of a model-free kind of sequence. Maybe that's why we can't reproduce the entire keyboard model or remember where each key is exactly.

I think it's a combination of both of those things. The whole point of becoming an expert at something, whether typing or playing a musical instrument, is that you figure out how to do it and you don't want to think about it. As an athlete, you don't want to have to think about how you do these things. You'll be faster and more accurate if you don't have to attend to all the details; you just want to play back some pre-learned sequence. Once you've done that, you don't need the model anymore. It's like an athlete who's a skier—they don't want to think about how they learned to ski, they just want to go with the flow and use muscle memory.

I guess what I'm saying is that maybe there never was an entire model of the keyboard. Even when we use the hunt-and-peck strategy, it's not like we have a model of the keyboard and we're thinking about that model and where the letter is. We're looking and searching for the letter. It's a partial model because we kind of know generally where the keys are, but the accuracy of placement of features in this case is too low. That's more of what we're thinking: there is a model of the keyboard, but it's just not that accurate. More generally, all our models are probably of that kind of accuracy, but we don't normally notice it because we're not trying to make such precise predictions. It's only when it's about which letter is next to which other letter that we realize we're not actually sure. This could be a big clue because we've always assumed that we do features at locations and that there's some ability to do any number of these things, but if you're right, it's an interesting observation. It suggests our models are really not that accurate, but somehow we learn to live with them anyway. I don't know what to make of it, but it's a really interesting observation.

I have another observation about this. The way we teach people how to use a keyboard—in fact, when I was trying to recall the keys on the keyboard, I went back to this method—we teach it, or at least I was taught, that you learn the letters in sequence going across the row. For example, the left hand is A, S, D, F, and then you have QWERTY on the upper row. I remember being taught that you can learn sequences, and we have no trouble learning very long sequences. We take the spatial keyboard model and turn it into a series of sequences, and then you can count your way across.

My mother, when she lost her eyesight—very poor eyesight—would start on the left side of the keyboard and recite the letters going across, then know which letter to press. That stuck with her until the end of her life.

Is there a difference between inference models and generative models? For example, if you speak a foreign language you haven't used for a long time, you can recognize the language, but generating it becomes problematic. We can easily recognize a keyboard layout and know it's correct, but generating is problematic. It seems like generating output versus recognizing it are different things, with different levels of accuracy.

I think we've talked about this before. Just because we can't reproduce a model doesn't mean we don't have a model. The better test is whether you could recognize if something is wrong with it. For example, you're very good at recognizing your dog, but if I asked you to draw your dog, or even just asked how long the nose is or where it's placed relative to the eyes, you're kind of bad at that. That's an interesting distinction between inference and recognition, and prediction errors internally to the learning network.

I've always assumed that the "what" pathway is mostly an inference model. As Viviane just said, you can generate predictions from it, but it's not designed to generate anything. People are terrible at drawing things as they see them. One of the main issues is that the learning module or cortical column has a model of the object, but we're not ever sending that model outside of the column; we're only producing action outputs based on that model. It's not like we can communicate this detailed model anywhere else.

Go ahead, Niels.

Oh, in terms of the connection to the current algorithms we have, you can imagine that if you're making a prediction during inference, you're essentially verifying that prediction. You can have a higher tolerance for noise because even if your prediction isn't very good, as long as it's in the rough neighborhood, you can still be correct when you get that incoming information. But if you're making a prediction and then trying to generate something from it, that uncertainty becomes much more impactful because it's not grounded by incoming information. When you're doing inference, sensory information verifies your hypotheses. If you're trying to generate, you're essentially doing a dreaming-type generation that quickly becomes noisy and unconstrained.

I remember I took a face drawing class for artists, and it was a serious course. What struck me is that even really great artists cannot draw faces easily; it takes years of practice because even the smallest error in drawing a face is recognized as an error. The tiniest error in placement or orientation is noticeable, and others will say it's not right. When you're trying to draw a face, it's hard to know how to do it. They teach you all these rules—tons of rules about where different parts of the nose have to be relative to other parts, where it has to be relative to the ears, and where these lines are. That's how people learn to draw faces: they don't just look at faces, they have to be taught an incredible level of detail. For some reason, we can't generate what we can see.

Isn't the simplest strategy just to turn an image of a face upside down and use low-level detailed models of local components if you want to copy it? That's easier if it's a photo, but that's still just copying. You're basically saying, don't be biased by what you think you know about faces, because whatever people think they know about faces is wrong. Don't think you're drawing a face; draw these little details. But that's not a generative model, that's just copying. If you have a blank sheet of paper and are told to draw a realistic face of someone looking sideways, it's really hard. You can't do it unless you've been taught and trained for years.

Back to your original question, the models we've been working with are not generative models. With a simple typing example, I quickly felt that to solve this problem, we have to rely on models in the motor pathway to execute the right behavior. I wouldn't go so far as to say those are generative models, but they're clearly in separate places.

Another way to put it: I don't think there are generative models and inference models in a single learning module. That seems unlikely. Generative models are used when you're actually trying to create behaviors. Those might be mostly in the "what" and "where" modules. But aren't the models at least generative in a basic sense? They might not be very good at generating, but you can make predictions; it's just that those predictions aren't so accurate that you can make a hundred predictions and produce a really nice image. Given some information, you can make a prediction to a next movement or whatever.

Maybe, but think about the issue with the tiny changes to a face that you see as wrong. That would suggest the predictions are pretty accurate. If it's just the slightest bit off, that suggests it's not a fuzzy prediction—it's pretty precise, at least in some places. But would you say we have any exact generative models? It seems like any column can output actions and generate something, but it's not going to reflect the detailed models we might have inside.

I don't know. I don't want to generalize too much because I don't think we can come to any conclusions yet, but I would say I have very precise generative models, but these are ones I've practiced. I don't know if I have any precise generative models that didn't require a lot of practice. I'm trying to think of some.

You mean in the sense that I can form very precise words and have exactly the muscle movements so you can understand me, but that generation of words doesn't mean that just because I can recognize words and understand what you're saying, I could speak them. We never really understood what V1 and V2 are trying to communicate to eye movements when they send projections subcortically. You can say projections per echolocus help move the eye somehow, but we don't really understand what the language is or what they're trying to do. I've never thought of the learning models we've been discussing as generative. I always felt that anything generative has been practiced a lot and is probably a more model-free kind of policy. It seems a little severe to suggest that, but I'm trying to think of counterexamples.

I had an ear training teacher whose approach was, let's say you're learning different musical intervals. You could have someone play them on the piano, and then you learn to recognize them over time. But his whole point was, if you can learn to sing it, if you can generate it, you'll be able to recognize it every time. Once you've developed the generative capacity, inference is 100%. Learning intervals, like in ear training. Yeah, I took a couple semesters of it.

It was true, it was a really hard way to go about it, but if you can generate the intervals, you'll never have any problem recognizing them after that. It is hard to generate intervals like that. If someone says, "Sing a minor seventh," it's really hard to do. Even major thirds are hard. What I do is sing a song that I know begins on a major third, and then I can sing a major third. That's how you do it.

I think we're exploring a big area here, which is about highly skilled practice behaviors. Even making the coffee cup is like that in many ways. I think it's very likely we can't just rely on the kind of learning modules we've been discussing so far. We'll have to have some sort of practice behaviors, too. I was thinking about my two-year-old grandson who's in the house right now. He understands many words, but he can't say them. Many of them sound the same, but he understands the words and thinks he's saying them, while the rest of us are left scratching our heads. It's interesting—he can't generate them, but he recognizes and knows them.

This kind of relates to what you're saying, Viviane, about not sharing models—the learning modules. It also feels like when you're drawing a face, so much of your model at that point is based on compositional representations, in terms of facial components. Even if you go to a point in that generative model and try to predict what was there, what you're predicting is a compositional subcomponent. It's hard for the motor system to produce, for example, an eye, which is why maybe going as low down the system as you can—literally drawing an eyebrow—works. You can do that well, but then the problem is you're so focused on that small model that you've lost the global model of where the eyebrow is meant to be in the broader face, and it ends up distorted. We're either engaging a very detailed model or a coarser one, and neither is really sufficient to draw a good face.

I'm not sure where this is going. Sorry. We should have time for your whiteboarding. I dropped the link into the chat. That whiteboard keeps sharing my screen, but you can go in there. What are we looking at now? This is the example problem I sketched out because there are a few things I wanted to clarify. One is—what tool are you using here? This is called ScalaDraw. I dropped it in the Zoom chat; you should be able to open it up.

Here's a task you're suggesting. Now you're switching to a new task for us to consider. It's the one I thought about, and I wanted to bring it to the group and see what you think. If you hit K, I think it's K, you can laser point.

Here's the idea I wanted to start with.

It's a hierarchical goal. We've got a lamp with a light switch, and we want to turn the light on, so it's already a compositional object.

There are several sub-goals to complete here.

First, we've got the current state of the lamp, which is off, as indicated by the gray background. Before we go on, can I ask a question? Are you imagining we already have a model of this lamp, or are you imagining that we don't? I'm imagining everything is learned. If this lamp was on my nightstand, I would know where to reach to hit the switch. I wouldn't have to look at the lamp at all. But if it was a lamp I hadn't seen before, I would wonder how to turn it on. Sometimes you can't find where the switch is. This one's obvious, but that's a different problem—a goal-oriented problem where you don't know the solution and have to find it, like maybe there's a switch and where is it, versus the one I've learned already, which feels more rote, like a learned task as opposed to having a goal and figuring out how to achieve it. But anyway, you're imagining it's something—well, that's a good point. I hadn't thought about that. I originally started with a light switch on the wall, which doesn't really change things much, but after talking with Niels, I put it on the lamp to make it a compositional object, to work more easily and clearly. There's no real exploration at this point. There's some learning in the next column, but if I already know this lamp, I have a model, and I'm not discovering how to solve the problem. I just know the solution and need to get my finger in the right position relative to the lamp.

Maybe in the model of this lamp, I know there's a switch I can rotate, and that's one of the features of the lamp. I know that if I press it, the light goes on, which is very different from encountering a new lamp and not knowing how to turn it on.

I'm not saying either one is right; I just wanted clarification of what you're thinking when going through this problem. That's a distinction I hadn't really thought of. I'm suggesting it's important.

I just named these "visual system part one." I didn't want to give them any concrete names to imply tiers or hierarchy, but essentially, these two things are not equal. The goal state of "lamp on" is transmitted.

Then there's this magic that happens, where you need to have memorized or learned that there's an association between the switch and the light bulb.

Down here—sorry? I was just going to say, for example, it would be more like the goal state being transmitted is "switch on," if the lower-level learning module models switches.

This is the important part I wanted to clarify: what are the goal states? Do they reflect what's happening at the upper level or the lower level? What's the domain? For the upper level, it's the lamp as an object, which can have a goal state—like the lamp being on. That's the state of the whole object it models. At some point, while exploring the lamp, the switch goes into the on state. That's a low-level object in a particular state. The lamp goes into the on state, and at that location—both physically and in state space—the switch is also on. There's a learned association there. As the goal state moves down the hierarchy, is it the lamp or the switch? I would say it's the switch, because the top box would be "lamp on," and that model selects, "for the lamp to be on, the switch must be on," and then it sends the goal state "switch on" to whoever models the switch.

This is a nice switch example because we're all familiar with this type, and it's not always clear which is the on or off position—it feels the same depending on which way it's rotated. To turn the switch on, you find the ramp and push down, rather than knowing which end is on. The model includes a lamp, and one feature of the lamp model is this switch. We have a model of these switches; this is one type. I know what it feels and sounds like. That is one feature of the lamp model, and the lamp model knows the different states of the lamp, all in the same learning module. The lamp is either on or off—those are two states. The switch is in one position or the other, which is also a state of the lamp. When you want to turn the light on—imagine it's on your nightstand at night—you know there's a switch, you know where it is, and you can find it, but which way to push involves the switch model. At that point, you feel the switch and know you need to push against the ramp in that direction. That feels like the switch model, not the lamp model, especially if you're not familiar with it. Maybe the switch is a three-way switch, so you can't say which way it will be when the lamp is on. There are two clear models: the lamp, which includes the switch and knows where it is, and the switch itself. When you rotate the switch—especially if it's a three-way switch—you have to invoke the model of the switch, feel the ramp, and know which way to push. I'm thinking out loud about the two states I would know.

One observation is that there's a model of switches, on and off. Now that Jeff pointed out just hitting a ramp on a switch, that model could be simpler: it's togglable. Instead of explicitly modeling on or off, you just need to toggle the lamp. You invoke a togglable action. This is not a switch where you know the on and off; the general model is you toggle it. You orient your finger to the switch, then you know what to do. If it's a particular lamp on my nightstand and always in the same position, I can remember which way to push, but in the general case, you have to invoke the model of the switch and toggle it. There is no on and off—just change it.

That highlights that it's not modeling a specific state, but more like an affordance, which feels like a different state space. I was trying to pick a switch with two identifiable states, but this one isn't. Typically, a wall switch is up for on and down for off, except with a three-way switch, where multiple switches control the same light. Then it depends on the other switch. That's what a three-way switch is—a confusing term, since it doesn't have three positions. You just have to toggle it to change the state, and you can't say what it is.

That's when you have two or three switches controlling the same light.

Anyway, it's interesting. I like this little switch. I think this example is fine, Scott. Even if we can't identify which set is on or off from the looks of it, we'll still be able to tell whether we changed the state of the switch after applying an action. I really like this example; it simplifies everything so we can focus on changing the state of the object.

Let me add a slight complexity. Sometimes these switches are on the back side of the base of the lamp. If you don't see a switch on the front, you run your hands around the lamp trying to figure out where the switch is. It could be up by the light or down on the back of the base. Let's say you're running your finger behind the back of the base and then you feel the switch. You don't know what kind of switch you're going to feel—you haven't seen it yet. There could be other switches, like a button in the back that you push in and out, or a toggle switch with a little lever. This is a more complex, goal-oriented behavior. You have a lamp, you recognize it as a lamp, but you don't know where the switch is or how to operate it. Now you have to find the switch, and you can either visually or tactually recognize the type of switch and then know how to generate the behavior for that particular switch.

Sometimes the switch is on the cord, so if you don't see it on the lamp, you run your hand down the cord until you find the switch. Sometimes those are toggles, and sometimes they're little rotating wheels with a click-on and a dimmer. We know all these things. It's interesting—you're discovering where the switch is, what type it is, and then you know how to operate it.

That's given that you already have models of switches and types of switches. You just don't have a model of that specific lamp. But you recognize it as a lamp and say, "There has to be a switch someplace." For that to work, it feels like there needs to be a learned association. For a lamp, there's a learned association between the change in the high-level object state (becoming on) and the low-level object state (the switch toggled). For a familiar object, there's a location associated with a particular switch that you can go to immediately. With a novel one, you can rely on the association between those states without needing a specific location. You know you need to find a switch to be able to toggle it, even if you don't know exactly where it is on this lamp.

I'm trying to think in terms of L6. For a familiar lamp, it would store an association between a particular location and the switch, but we can't rely on that entirely because we can see new lamps and still understand that we need the state to change. It's a different learned association, or maybe a complementary one.

It's a great problem scope because it can keep getting more complicated. When we recognize a lamp, there seems to be a behavioral model for lamps that might be independent of the lamp itself. It brings up the question: is there a behavioral state model that can apply to different objects? Many different types of objects need to be turned on and off, so there's a general state of "on" and "off," and there must be a switch somewhere. That applies to different objects; I don't have to learn it on an object-by-object basis. I can apply similar strategies to different objects. I can have a very different type of lamp and go through the same search pattern—maybe it's on the back, maybe on the cord, maybe behind the bulb.

This is a good problem because it might involve the need for state models and behavioral models independent of particular objects. It's also nice because we can start very simply, knowing a specific lamp and focusing on the motor decomposition part, then make it more complex by learning how to turn on any kind of lamp and modeling any kind. Or maybe we do it the opposite way, Viviane. If we do the hard part first, we have to be careful that we don't just solve a trivial version that doesn't apply more generally.

It depends on what we want to focus on first: the motor decomposition and goal state part, or the lamp recognition and behavior modeling part. It's a great problem—very isolated, with lots of variations and compositional structure in behavior components. With the behavior of the switch, we have different types of switches. As we're searching for the switch, we know we're looking for a switch, but we don't know which type. How do we search the space of switches only?

We're not just moving our finger around recognizing anything; we're looking for our switch, yet we don't even know what the switch will look like. I've been thinking about half-complete columns—if you take state but strip it of some details, can you create a representation of an abstraction by removing details in the location or state space? That was the suggestion: you might have a state model and an object model, separating the behavioral states of an object from its physical state at any point in time.

It would still be in the same column, just with well-defined activity in the superficial layers. That was purely speculative on my part; I don't know if it's true, but it was a nice idea that you could have object models and a set of state models in the upper layers, and object models in the lower layers, in a single learning module. Then you could mix and match state models with different object models, which seemed interesting. We could do all that in one learning module, but it doesn't mean it's correct.

One reason I suggested that is if you separate them—one learning module learns objects and another learns states—I'm trying to preserve Mountcastle's principle of a common cortical algorithm. That would mean I'd need columns with the same architecture, but some would do state modeling and some object modeling. It's possible, and then I'd need a way to mix the state models with the object models in an efficient circuit. I always try to do as much as possible in a single cortical column, so I combined them. That could be wrong; it doesn't have to be right. It just feels good.

This is the last step. We have two states: our current state, which doesn't match the goal state for the switch. I can revise these, but essentially, the association is made at some point, and we have this delta between finger here and here. The job is to minimize that delta, bring it to zero. That might be analogous to the delta between switch off and switch on in state space—the delta we want to collapse to zero. I'm not sure if it maps so nicely into abstract state spaces, but bringing up the idea of toggling makes it interesting to think about.

It seems that as the goal state, we wouldn't transmit the actual model of how we want the switch to look, but rather that we want to apply a certain amount of pressure to that ramp. The motor system can bring the finger there and apply the pressure, but it could also use any other part of your body or even a pen to flip the switch. The goal is not to have one end of the switch; the goal is to toggle the switch. Unless the switch is already in the on state, you might need to change the bulb or something. How would I know? The first thing I would do is check the on state. If I saw a lamp like this and the bulb wasn't on and I saw a switch, I would flip it. If it didn't turn on, I'd flip it back and forth a few times, wondering what is going on. Then I'd check if the lamp is plugged in. If it is, I'd make sure there's power in the cord, and if that's working, maybe it's the bulb.

The point is that to switch the switch is not to get it in a particular position; it's a behavioral action to change its position or state. We don't know which way it's supposed to go. If I know this specific switch, I know which side to press and the goal is to have that side down. If I didn't know this particular switch, I'd have to figure out which is the upside and toggle it. If it was a lever, I'd flip the lever; if it's a button, I'd push the button. All those are state changes as opposed to physical changes.

It does feel like, to a certain degree, we do model if the switch is on or off, but it's independent of the morphology and inferred from more than that. If you saw the lamp was off, you would infer initially that the switch is off. But if you turn the switch on and the lamp is still not on, now you're in a state of uncertainty. Those are all correct words, but I'm not sure how it feels to me. If I was doing this, I wouldn't think "on" or "off." I would just find the switch and toggle it. When you're familiar with it, it's different, but if I was really trying to understand, like if someone said, "This lamp isn't working, can you figure it out?" I'd be thinking much more about the state of the switch and whether there's power going to the sockets, or if I'm changing the light bulb.

You might have an explicit model, but I'm not saying you always need that. I feel it's the opposite. Imagine I have this switch, and my inclination is just to change it. It's not like I know which is on and which is off. I just change it. If the light didn't come on, I'd go back and forth, clicking it repeatedly, wondering why it isn't coming on, not even thinking about which position should be on. By changing the state of the switch, the light should come on somewhere. It's a subtle detail, but at this point, I don't even know which is on and which is off. Unless I saw a one and a zero, then I would know, but it's a subtle difference—not worth arguing about.

It seems like the switch can't tell what state it's in; only the lamp model can tell whether it's on or off. The switch model can only tell you which side is tilted and where to apply pressure to change the state, which the lamp model can then recognize as a change in the lamp's state. I came up with three different switch types: a toggle, a ramp, and a push button. In all of them, there's not an obvious on or off state. I just need to change it, and the expectation is that if I change it, the light will come on. In a particular lamp, if I learn that lamp, I might learn the specific direction for on and off. But for an unknown lamp, the goal is just to turn it on, and I haven't learned it yet. It's more this concept of toggle—Tristan or someone suggested that, and I liked it.

I think the state change concept is important, and I had to think about it a lot when working through this problem. State changes seem to be very informative. Can a goal state be a transition? Can a goal state be a state change? We've been thinking of the goal state as an end state you want the world to be in, but sometimes it seems like your goal is the transition. I don't think it's the world; it's the state of a particular object. The object could be the table setting, the coffee pot having coffee, or the lamp being on.

In the case of a toggle, after you've completed the goal of pressing it, the state of that button is the same physically and in every way. What do you mean?

If I flip a switch up, it now has different physical characteristics than if I switch it down. This is easy to understand if you know the orientation, but if you don't, it's the same in either position. If the switch is on the wall or somewhere else, and it's symmetrical, one side down and the other side down look exactly the same outside the context of a particular orientation on an object. You can't look at the switch and know if it's on or off. The same applies to a toggle switch. Imagine an elevator button—it would be identical. A button that doesn't change, except maybe it glows afterwards. If you have a remote and push a button, afterwards it looks the same as before. The goal state for the button might just be to go through the sequence of being indented and then returning to its original position; it doesn't need to look different afterwards.

We clearly have models of switches. I have a model of this type of switch because I know about it. I have a model of toggle switches and push button switches. When we're searching for the lamp's switch, we don't know where it is, whether we're searching visually or by touch. We know we're looking for a switch. Somehow, we bias the search algorithm for a switch, and when my finger goes around the back and I feel something, I know which type of switch it is. Once I know the type, I know its behavioral model and how to interact with it. Until I find the switch, I have no idea if I'm supposed to flip a toggle, push a button, or push a ramp. We can decompose the problem: we're searching for a switch. Somehow, your brain has to say, "I am searching for a switch." It doesn't know which switch, then it finds something and says, "Yes, this is a switch," and then, given this switch, I know its behavioral model and how I should interact with it.

Maybe that's why so many lamp switches are just an action you have to perform. If it's a chain, you pull it; if it's a button, you press it. This eliminates the need to check the state of the switch. As you pointed out earlier, most wall switches—at least in the United States—have an up movement to turn on and a down movement to turn off, unless it's a three-way switch.

If I wanted to turn a light on in a room and saw two switches, one down and one up, I would first try the down one and flip it up. There's an exception for wall switches; we have a different behavioral model for those. We have to account for all that somehow. My parents have a lamp without a switch; you can touch the lamp anywhere and it turns on. I was very confused the first time I saw that lamp. They have those lamps in hotel rooms with a rotary switch that sometimes takes two turns to turn off—the first click does nothing, and you have to do it again to turn it off. Those are the worst; they should eliminate those switches because they're so unreliable.

There's also the clapping one—you can do it anywhere in the world. I'm just wondering if everything is actually happening in the affordance space and is based on environmental feedback. We then say, "Oh, it's a switch," etc. The lamp is on or off, and I want to change it. I'm seeking to effect an associated change that I learned from experience, and everything else is incidental. Maybe that's true, because I could come across a switch I haven't seen before, but it's also true that we can very quickly recognize any of the pre-learned switches. So, maybe what we're searching for is an affordance.

Mentally, I'm thinking I'm searching for a switch. Maybe I'm wrong and I'm searching for an affordance, but I have certain expectations about where the switch would be. If I don't see it, it might be on the back of the base, the back of the lamp, or on the cord. What makes me think about affordances first, and a switch as a learned, more concrete behavior, is that if it's a completely unknown object, you'll just mess with it, affect change, and see what changes you observe. But here, we're not talking about a completely unknown object; we're talking about an object we know is a lamp and has two states: lit or not lit. If I'd never seen a lamp before and didn't know it produced light, then you're right—we would just pick it up, play around with it, maybe stick a finger in the socket and get a shock. We could think about that problem, but as it's posed so far, it's a lamp, we know it's a lamp, so maybe we don't want to start there.

I think it's a good point. Maybe you've never seen a lamp before, so how do you learn what lamps are? You might play around with something and see that the light comes on. There are a lot of kid toys like that, with brightly colored buttons. If you press any of these buttons, a light comes on and some sound or music plays. The kid learns that pushing a button causes something to happen. They don't know this in advance, but they learn by interacting with the toy. We have a lamp where you just have to hit the top, and it cycles through four levels of brightness. Link loves this lamp and figured out quickly to hit the top. He's not dexterous enough yet to flip switches, so this lamp is perfect—without fine motor control, you can go through different states. He learned it very quickly. We did the same thing with our two-year-old grandchild. There's a light over our kitchen counter with a switch on the side. In the beginning, we told them it was magic. We pointed to the light and said, "Jamie, say on." He says "on," and we flip the switch. Then we say "off," and we flip the switch. If it didn't work, we showed him the physical switch, and he got it right away. It's interesting—you don't really know this stuff until someone teaches you or you discover it accidentally.

It would be helpful for me if I could leave soon, because I have a lot going on. I think this is a good problem. Scott, Niels, whoever worked on this—Scott, I can drop this in the TMP research channel if you want to look at it later or if it changes. The last bit was me trying to learn the association between switch states and lamp states in the model where columns have state and location. That's the last bit here.

I still think the idea—I'd love to have, and maybe you have it here—that we have models of switches.

Imagine a toy problem where we have a model of three different types of switches, each with its own behavioral model. Then we create a model of lamps. Somehow, when we see a lightbulb, there has to be a switch. I don't know how we do that, but we're looking for the switch, and when we find it, we decide what type it is, then we know what behavior to use. This deals with a lot of uncertainty. It's not all learned—we don't know what the switch is or what type it is. It's a very simple version of the coffee pot problem: you're trying to achieve something and searching for the solution.

So, just real quick, I wanted to take this abstract diagram we had on the left and map it onto columns with state and location. Part of that was learning the causal relationship between the switch and the light, and what that might look like. I started simple—there's no motor system.

At T0, lamp off, switch off. At the next time point, the switch goes to the on state, and there's a discharge because of the state change that propagates up to the lamp. Let's say the lamp also goes on instantaneously, which is basically the case. We get our typical spreading up into superficial layers, so there's a connection forged between the state here and the state here, as well as information about the switch module or column. When I want to invert the problem and use my memory to decide how to turn off the lamp, I know where to go on the lamp to do it. In the next time step, this state has also changed. I didn't want to draw everything on the same diagram, but since this state has changed, there will be some emission out of here, just going nowhere, but also leaving the deeper layers and arriving down to the lower part of the hierarchy. Because of this connection, we end up getting a full association between the state changes and the two modules. We start here and end up here.

In a vague, hand-wavy way, maybe this loop of connections is the mechanism—somewhere in there—that allows us to create this process: the relationship between the states in the lower and upper columns. That's the extent of it.

I think that's a pretty good start. The only error I'm confused by is the dotted arrow in T1 between L23 and L56. What's this one again?

That represents the fact that we can assume a certain amount of columnar communication across the column, so there can be some intermixing between what's going on here and here. The reason I drew the arrow is that if we take this path—here to here, these deeper layers, and then back to here and here—if we don't end up here at some point, then we don't have the top state of the module, or we don't have the state of the lamp. We need some kind of communication along this axis for these projections to contain state about the lamp.

I think that's definitely true. Although the major input to L3 is from L4, in general there are reciprocal connections. Sorry, hang on. I'm not sure. Did I change something?

There's a cursor visible. I think you've moved; you're moving an arrow. I don't have anything in my history of changes, but let's see. Maybe I'll just draw it and then reverse everything. At least how we talked about it so far was that there's a bidirectional association between the features and locations. The actual state and ID of the object are just inferred like this, but there's no backward connection. I think before the changes, you had both of these arrows bidirectional. I'm not sure why there would be a connection down again to A5, A6, or what exactly the difference was between the dashed and solid arrows.

I just left the solid ones as the ones that are in the diagrams that we all know—the way input comes into L4 and then spreads out from there. Those are the solid ones, and the dashed one I added because if we want state information to exit, lamp state information to propagate to here, then we need the dashed one. It's dashed as in speculative or required in some way for lamp state to make it down, to make the association with switch state.

Why would that require the connection? I'm thinking about this circuit: the leaves two, three here, go up here, and then eventually it's going to leave this layer and come back down here, right?

Leaves the deeper layer, lamp. There's a deeper layer switch, and then it does have this ascending connection that synapses out here.

In order for there to be an association of some sort between the switch and the lamp, I need to get information that's up here to exit from here.

I don't think we need that. Sorry, Niels.

I was just going to say, I think there is evidence of connectivity from L2, L3 to L6.

I agree that we want to condition L6 and probably L5 on the state of the object. The location space is already unique to the object, so the location already tells you something about the object ID. We don't need an explicit connection from L2, L3. But if the state, like Jeff was saying, is a separate representation, and if that can influence those things, then maybe that's how that works. You mean the state is in the same location space, but wouldn't the state then also be different locations?

I guess it depends on how we implement it, but one thing I was thinking is that the way you path integrate over the reference frame is conditioned on the state.

Here, the point is that we have the goal state in the higher level, L2, L3. Depending on that goal state, the only way we get feedback from the higher level column to the lower level one is the projection from L6, so depending on the high-level goal state, the information projected from L6 is going to be different.

Didn't we talk about the goal state coming from L5?

It depends on whether it's hierarchical. If it's hierarchical, it would be from L6. If it's a direct subcortical one, it would be from L5. You said from L2, L3, but that would be the actual state. That's what's in the higher level one, and that's what's influencing what goal state—let's say the goal state is "unscrew light bulb" or something like that. We're going to want to predict a different thing; L6 is going to want to project a different thing to a lower level learning module.

Within the learning module, whatever the output goal state is, it's going to be conditioned on its own internal goal state. If that's represented up in L2, L3, then yes. The other thing is that the actual state is L2, L3, the goal state is more in L1, and then it's just whatever is sending apical dendrites up there. That's how it's being communicated down.

L1 doesn't have the descending action outputs to lower layers. L6 could have projections up there. Maybe it's Kira. Definitely L5 does. I'm getting a bit confused by the different L numbers. Definitely L5 has projections up. The way I was thinking is that although L6 is the only way to send hierarchical connections down from one column to another, if L5 is the motor source, it has connections to L6. Maybe it's the one that's saying, instead of predicting "you should be seeing this," it's somehow changing those down projections to "you should be enacting this; this is now a goal state." I think that's what you're getting at, Scott—that we need to bias the output goal state by the target module.

Yes, eventually. It's a lot of states and L's. I think everything you explained on this figure, Scott, I agree with, and it's a really nice drawing. I'm a bit skeptical of the dashed arrow, especially going down from L2, L3 to L5, L6 within that column and why we need that right now. We may or may not.

I was trying to think about a way to make sure that whatever state information we have about the lamp is accessible when information leaves layer six.

I think we can definitely—oh, sorry, go ahead. In the case of a lamp, there's no morphological change between the two different states, so it seems like it couldn't be entirely encoded by whatever is in the space of the lamp. It also needs to get some features.

There's probably a variety of ways anatomically this could happen, but maybe the most reasonable one is L5, which has a lot of apical dendrites up in L2, L3. It gets a lot of input from there.

If L5 is generally where a lot of motor stuff is happening, it has a lot of reciprocal connections to L6. That information gets transferred down to L5 from L2, L3, and then L5 is somehow biasing the output of the hierarchical connection from L6. It seems like it must exist anyway, because what the state is has implications for what the location representation is. These locations are different when this is an off state than when it's the on state, so there's got to be some communication between these layers.

There's a connection between the state and the location representation, but it's not entirely clear how it happens. Could that correlation be looser if the state doesn't model state directly, but instead a point in that state space is just a dimension you can change—togglable, smaller, larger—versus an actual discrete state like "large," "sizable," or "togglable"? Then the correlation wouldn't need to be as strong.

So, a contained state space versus a discrete state space and how that relates to the location representation? Something like that?

I think so. I don't have very refined thoughts on that yet.

I'm not sure. Maybe I didn't understand, but in general, it's probably fair to say that we're very uncertain how we're going to model states. Like Jeff was saying earlier, maybe it's two separate things, or maybe there's even a whole separate reference frame for state space. One thing that's for sure, and this is already the case with the goal state generator, is that it has an output goal state conditioned on its own driving goal state. That seems essential to communicate that information. I think that's a fair thing to show, and then we can debate how it's implemented. My problem with connections from L2/3 downwards is that L2/3 wouldn't know about the state of the lamp if it didn't get the upward projection. If the lower layers haven't already inferred the state, then L2/3 has no information about whether the lamp is on or off. There's no point in sending it back down, because it just got that information from the features and locations.

Maybe it's clear that we probably want this separated in the neurons as well, to separate out the actual state and the goal state.

That is presumably going to be different. Alpha 3 represents the actual state, not the goal state.

Well, we're over. I'll let everybody go, but if you feel like messing around with any of this or making your own diagrams, here are all the assets—you can just copy and paste, come play in the canvas. These are really nice figures. Thanks for putting this together. That's awesome. I really like that example you came up with. Thank you. It was good for me to sit there and try to get some things down more concretely and think about it more carefully. It's great. Thank you. Thank you.