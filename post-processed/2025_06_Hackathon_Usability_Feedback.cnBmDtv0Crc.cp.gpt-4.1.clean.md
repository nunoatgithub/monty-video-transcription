Okay, put a recording. We wrote down all our feedback in the hackathon code feedback document that's linked, and we all voted on what we're going to discuss. Looking at the votes: two votes, two votes, two votes, two votes, one, one, three votes.

First item we're talking about is: I can't use my own sensorimotor ID convention and I have to split one sensorimotor into two that use top depth and RGBA suffixes. This was an example I had to do here. Initially, I was just returning from the environment state a sensorimotor that only had a patch, but I couldn't do it because patch depth and patch.RGBA were hard-coded expectations for the sensorimotor state, even though, as you can see, they're fully coupled to each other. There's some assumption built in here. That's the problem I encountered.

Viviane, you have two other items above that seem to refer to the same issue. Which one are you? I guess it's not the same issue, but there were two others referring to confusing naming conventions, like calling things Patch or Sensorimotor Module Zero, and implicit assumptions in other places of the code that you're using those names, which seems maybe related. One comment now that you showed this code snippet is that I totally agree it's a problem, but it's only a problem if you're just customizing a certain part of the code. For example, when we wrote our hackathon project, we were able to return state with no RGBA or depth because we have ultrasound, so we just called it ultrasound. That was possible because we also wrote a custom sensorimotor module class that didn't expect those. I think the main place where you probably encountered the issue is in the depth to 3D locations transform. In our project, we didn't use that transform, so maybe that's why it wasn't as big of an issue. Do you remember where that expectation was set, that it has those two fields?

I think it was in the depth. Yes, depth to 3D, if I'm right. That sounds right.

I can't remember where that was. Okay.

Then the question is, how general do we want that transform to be? Generally, that transform requires a depth channel to do anything, and its purpose is to transform the RGBA channel and return that in a different format. So it requires those two fields.

But it wouldn't necessarily need them to be called exactly that. In some other places, we have variables you pass to the function that specify the name of the field you want to use for the depth channel.

The other thing here, structurally, is that you're given a dictionary and you're hard-coding a key. That's the coupling, like you just said. This pattern happens a lot in the code. Feedback would be to avoid this pattern of magically knowing what property to access on an object that was passed to you. Have whoever's calling you make it their responsibility to give you the right thing, especially in a generic dictionary. That's the pattern to avoid.

I had some questions about how generic these transforms were supposed to be because they get my experiments specified here. This is what a flat configuration looks like.

Never seen in the wild before. In the transform, it tried to be the same for our ultrasound project this time too. That says a lot about usability and configurations. Both teams went with the flat config. This is a dataset configuration. There's a transform happening on the dataset, and this happens inside the get item implementation. When we tell the datasets to give us something, it gets an observation and state, and then it transforms the observations themselves.

Shouldn't this be happening in the sensorimotor module or something like that? For context or history, first, I'm not happy with this and I agree that by how we think about sensorimotor modules, this should be happening in the sensorimotor module to transform incoming data into the CMP and get the world coordinates. The reason we have this kind of dataset setup that applies transforms is because when we first started writing this codebase, we used Torch conventions. In Torch, they have dataset, data loader, and transforms, and it's exactly that kind of setup. The original idea was that this would make it easy to plug in a deep learning system and compare to that, but we never really did that. We did it with vector neurons once, because deep learning systems aren't really made for interacting with environments and learning continually. We're not learning from static datasets, so we set up the repository for static datasets even though we're not working with that.

Long story short, I'm not a fan of it. To me, what I'm hearing is the transforms themselves will become part of the sensorimotor modules. That's part of the refactor of moving away from dataset, data loader, and finding a different place for them. That makes sense. I'm tracking that. I was going to say, we already have that noted somewhere. Otherwise, just want to make sure.

Does anybody else want to discuss this further? Let's go to the first one with two votes. Hojae and Jeremy voted for Embodied Environment as abstract classes that seem too specific to simulators: add object, remove objects. I'm going to share my screen. I happened to be familiar with the previous one. Let me know if you want to discuss this.

I think I added this item because when I was customizing the embodied environment, I had to write these boilerplate classes because they're in the abstract class, even though they're just for Habitat.

I might have been the one who actually added it, but I don't remember why it should be part of the abstract class. I think it should not be.

I ran into a very similar thing. If you want to show your code, go ahead. I just remembered working on this as well, environments.

I just skipped that environment because the problem was that the environment has abstract classes specific to simulators. I inherited directly from Embodied Environment because I didn't want the add object functionality. We implemented everything, so it's also an environment, but we went directly from Embodied Environment. Maybe I'll share my screen to show what we have.

We also inherit from Embodied Environment, which I think is the abstract class, but then it requires you to implement add_object. I see it—raise NotImplementedError: Ultrasound Environment does not support adding objects. I saw this pattern in a couple of other custom environments too, not just Habitat.

I scrolled down and saw that I put raise NotImplementedError myself. I don't think you shared, so I'll reshare. Yes, this is what you're talking about. Exactly, we just needed to raise NotImplementedError. It's too coupled.

It feels like this is definitely a simulator. Maybe it's just coupled to a simulator too soon. Maybe there should be a more generic environment. I think we have a Habitat Environment class that inherits from Embodied Environment, and that would be the place to have those, or to have a Simulated Environment class.

What comes to mind is that we should probably just have Environment, then Simulated Environment, and then maybe Embodied Environment. The Simulated one could have add_object, while Embodied Environment could handle things like positioning. This makes sense to me.

See my notes. Jeremy, if you have comments since you put your name there. My thoughts are more about how things are often too tied to simulation environments, and we discuss that a lot. This is good.

Looking at that pattern reminded me that I've never actually used abstract base classes. In Bitbucket, we would have a base class, manually implement the methods, and have them throw NotImplementedError, usually pulling the class name so if you subclassed it, you'd get a message saying this class doesn't implement it. That might be something you could do here—make those methods optional.

But I think I like the idea of embodied versus simulated better. The reason we have these abstract classes is because we want Monty to be modular and customizable, so you can write your own learning module and plug it in with existing components without changing other stuff. The abstract class gives you the boilerplate of the functions that other components expect, making it clear what you need to implement. What you describe is like a protocol—defining the interface or template. The difference is that an abstract class fails on instantiation, while a protocol fails at runtime. That's the trade-off, and it's arbitrary depending on our tooling. For protocols to be useful, developers need type checking as part of code development, which we shouldn't require right now since Python typing is still changing. I put it in for the same reason as Jeremy.

I had one more thing to say, but I forget. Is the abstract base class also used for tests? I don't know testing well, but I know about mocking objects and fake buffers or agents. Do those require abstract classes? No, you can mock any class. In Python, it's all duck-typed—if the method isn't there, you get an AttributeError. Abstract classes are more of a strongly typed language thing. Fakes aren't there to wrap abstractions, just to provide fake implementations for testing. They'd be there without abstract classes, just to help the test. Thank you.

I still see value in having abstract classes for now because it's all in one place. It tells you what's expected of the class—an environment should implement step, reset, pre- and post-episode functions. You see it all in one place. We could add more documentation to explain what these functions should do. I agree, it has value, and maybe it just needs to be more abstract, with Simulated Environment having more specific things.

Let's move on to the next topic with two votes.

There are multiple libraries used for defining orientation, like SciPy Rotation and quaternions. They use different conventions. We have Slack threads about this now. The ordering can be different—scalar last or scalar first, with W as the scalar and X, Y, Z as the imaginary parts. This can be confusing and lead to errors. Rami Hojae, whoever wrote this, discuss.

The order is slightly different, and there's also the problem of having multiple libraries doing the same thing. I don't know why we have SciPy rotations and also quaternions. Quaternions seem to be defined in earlier parts of the code with the Habitat simulation. Maybe it's required, but I'm not sure. The NIA package is like an extension of NumPy to allow for something—I think it's a very janky one. I looked into this and wanted to get rid of it because I think we only use it in a few places. I think we use it in the interface with Habitat.

I'm not sure. I think Louis wrote that part of the code. I believe it does, but we could probably move it further towards just the interface with Habitat. I'm not actually sure, because I know Habitat has its own Coran thing in their C++ implementation. I don't know if it's essential. I don't think Habitat requires it, but I think we have it so that we can transpose things to put them in the right order. We may be able to get rid of the dependency, at the cost of clarity. That package specifies a format, but we could manually implement it. My reason for bringing this up was to decide on ordering as a group, so we can stick with it. Everything would be converted to that, so everyone knows anything coming out of Monty will be in that ordering. If they need to visualize using OpenGL, they'll know which ordering they're getting and whether they need to transpose.

I know OpenGL and Blender, and visualization tools and game engines like Unity use XYZW, while science libraries like ROS and SciPy use WXYZ. Wait, isn't SciPy XYZW and NumPy is WXYZ? Sorry, I might be forgetting—it's already confusing. Jeremy, you made a good point on Slack: the more intuitive way is to have W first, but if SciPy is the main thing we're using and it's everywhere in the code, we should probably stick to that convention. The main other instance we might use is 3D graphics software when exporting. Maybe we say XYZW, but make sure it's really clear. My vote is for WXYZ, because then the array is WXYZ, which is easier for math. If we could do that, it would be very clear every time you're calling SciPy which convention is being used. Even someone who doesn't use SciPy would know. "Scalar first equals true" would be the keyword to add. That sounds like a good approach. The reason I'm going for WXYZ is it's easier for math.

I agree, it's definitely better for that. I just didn't want us to have to specify it every time we call SciPy, but that flag sounds perfect. The balance is that having it easier for math is more important than for visualization. If you have a strong case that visualization is the main issue or more difficult to solve, I'm open to that, but I don't think it's that important. My only remaining concern is if it adds computational overhead to keep converting, but as far as I remember, we just get the Coran from the observations, and once we're inside the learning module, we don't work with them anymore, which is where all the heavy lifting with rotations happens. It shouldn't be a big issue, even if we have to convert once. Transposing isn't nothing, but it's not what's slowing things down. If we had to do it for 10,000 hypotheses, it might add up, but when we're testing, we're working with rotation matrices, so it shouldn't be an issue. I'd be surprised if SciPy supports an argument for it and doesn't handle it gracefully. I think the person this will immediately impact is Jeremy, as he's working towards Muko work, so he might have to deal with that. That one looks like it's WXYZ as well.

Another password for Muko, our code.

Not in this meeting, but I'm excited to tell you what we're doing with Monty to make it work with Maka. Jeremy should give a presentation at some point about what we're thinking, because that's exciting. It's almost obvious what we're doing with Monty. This was a teaser—a trailer is coming later.

The next vote item is some abstract classes. Are these needed for testing?

Maybe for code, if not also, but multiple inheritance makes things harder to understand and more difficult to extend for other users. More often than not.

I added this, but I think this is addressed. Abstraction promotes stronger coupling compared to vertical approaches, but that's why I added this. I think we also talked about this. What are your reasons for putting your names?

I think we already have this in the list of undesirable things, but I thought it was worth resurfacing. Right now, with the different types of learning modules and graph classes, it's really hard for new people to read the code, or even for people familiar with it. A lot of that could probably be collapsed into a couple layers of inheritance. A more extreme approach would be trying a more modular design.

Sorry, that wasn't really discussed, but that's what I was thinking. I wrote after Ness, and then my mind went to multiple inheritance. The abstract part is addressed and fine. The multiple inheritance—Niels and I experienced this when we were trying to write a data loader for DMC. It was a crazy kind of gotcha.

Let me ask a clarifying question: when you say multiple inheritance, do you mean...

Multiple levels. Height. Yes. Okay, so that's not multiple. That's okay. This is the intent inheritance hierarchy. I'm going to switch that. Thank you. That's also what I was thinking of: deep inheritance hierarchy. For the learning module, I think we addressed this already, Neil, when we did the refactor two years ago. Before, we had all the learning modules inheriting from each other: the abstract test, the graph learning module, the displacement graph learning module, the feature graph learning module, and the evidence graph learning module. It was a six-part hierarchy, and then we refactored it so it was just the graph learning module and then the three types from the graph learning module. I think we can flatten that hierarchy as much as possible for now. I agree for some of the other classes, like the data loaders and maybe the experiment classes, there is still some nesting that might not be necessary, or policies—I'm not sure. Maybe it's not necessary. We could flatten that more if we got rid of the feature and the displacement one. To maintain the ability to go lateral, we still have that. But I take your point; it's definitely much better than it was. We just need to think about other big classes, besides learning modules, like the sensorimotor modules, how they're connected, data sets and data loaders, and go through the same process.

For what it's worth, my goal is for every inheritance hierarchy to be one level deep. You have an abstraction and you have instances of it, and that's it. Everywhere in code, I will be attempting to flatten the hierarchy to that always. That's my intent with policies and positioning procedures, for example. I intend to flatten the policies and also to flatten the positioning procedures.

I think that makes a lot of sense. That's the challenge: as soon as you go deeper than that, it becomes hard to think about.

For me, it would be useful if it's not too much work to have a meeting where you could give some tips on alternative patterns to use. At least for me, I default to hierarchy sometimes when I want to reuse something from a class and just customize a function. If you could present what would be the alternative, that would be better for us to use. A general quick first approximation answer would be a mixin. If you're trying to share some functionality, rip it out as a mixin. If you have a class and there's functionality you want to reuse, instead of making a subclass, think about how to rewrite it as a mixin, mix it into your original class, make sure everything still works, and then write a peer class and grab that mixin and put it in there as well. That would be the general pattern to reuse functionality. Rami has been doing that recently.

Is it easier to do something else? Inheritance, for me, is part of the class definition. Instead of passing as a class definition, you could pass as an argument. If I need some functionalities of, say, a visualizer class, then if my LM needs a visualizer, one way is to pass in a visualizer instance as an argument and just call whatever function we need from that class. This is called dependency injection, right? Dependency or compositionality. That's actually the better way. What JJ said is probably the most preferred way: compositionality. I would go with what JJ said as the default.

Just to clarify the difference: with composition, how is it injected? I'll post an example; it'll be much clearer. It's very easy to understand, so I'll just post an example in Slack. That would be great. I was trying to think of something in the codebase, but my mind is blanking.

With the very first question at the top, you suggested that the abstract class could have another class, like a simulator environment, from which another class would inherit. Is that a case where hierarchy is better because it makes it clear to distinguish types of classes, or would you also use composition there? I would use hierarchy. Environment would be abstract, simulated environment would be concrete, and nothing would inherit from simulated environment—it's flat. 

If I want to have a habitat environment, I would create a habitat environment. What is simulated environment for? It sounds like it's general to any simulator. I think this is abstract. We have an abstract environment, an abstract simulated environment, and then habitat would be a concrete implementation. Simulated environment does not inherit from generic environment. Maybe there's an environment protocol; we have to think about it. That's a great example—specifically, what would we do to implement habitat environment? I'm not sure if this is standard. It would be interesting to look into this more, but if the second level of inheritance is still abstract and just defining methods, it still feels easy to read. I could imagine us having a very abstract environment that covers everything, including simulated environments and maybe robotic ones. Then we have simulated environment, which should have add object and similar methods, but they're not implemented. Technically, there are three levels, but the third level implements everything concretely. When you're reading the third level, going up a level isn't about understanding what a function is doing—it's just the structure or skeleton of the class. The top level tells you how it relates to others, but maybe it's not even helpful to have that top one.

Thinking about the answer, because this is a rule I really don't want to break, what I might do is have habitat environment inherit from environment and also implement the simulated environment protocol. That makes it one deep—just an environment. Simulated environment also has an additional protocol that these environments follow. When we have an experiment class for running on a simulated environment, it would check whether our environment follows the simulated environment protocol. If you pass an environment to experiment and it's not a simulated environment, it would indicate something is wrong.

I just want to highlight the importance of not breaking the rule, which forces these types of solutions that end up being more intuitive. When you hit a wall with a rule, don't give up on it yet—it will guide you to a solution. Once we say, "let's keep this one level deep," we can find another way to solve the problem. The reason this is preferred is probably just familiarity, but it still seems to me that if it's a hierarchy, it's easy to follow the path. If there are multiple things, there's a fork, and I have to decide where to go first, then come back and go to the other place. I could argue that it's harder to follow. Niels and Hojae said the opposite. For me, the difficulty is if the parent class is also making concrete implementations of methods, it's more likely to get confusing because some things call super and some don't, leading to a lot of back and forth. As long as the parent is still abstract and just adds abstract methods, which is what makes it unique and a simulator thing, that seems fine. Maybe, as Viviane says, it's just familiarity that makes it easier to read and understand. I'm happy to stick to the rules, and if it starts not to make sense, just let me know. Ping me or Jeremy, and we can have a conversation. It's difficult to give generic guidance; it's much easier to do it specifically on the problem, because then it's easier to provide a solution for a specific instance than to talk in generalities and then apply them.