Nice. So yes, everyone may have seen on Slack the plan. Today, Ramy is going to present some slides and we can discuss ideas around that. If there's time at the end, we can open up to more general brainstorming.

Okay. Whenever you're ready.

Do you want us to ask questions from the start, or would you prefer us to wait? I think it's fine. Go ahead and ask questions. I'm mostly unsure if my understanding is correct, and I think the best way is to have this be interactive. If I make a mistake, just let me know that's not how we think about it. Cool. All right.

Can you see my screen?

Yes.

I'm going to be talking about modeling object behaviors, and I'm going to propose a slightly different way of looking at it. I'll be starting from the same current theory we have, but it's going to change in some details. I'm hoping to have some discussions on this.

Let's start with the current theory. We have movements that are coming from the thalamus, relayed by the thalamus. From magnocellular pathways, some of those cells have larger spatial receptor fields that represent the sensory location or the sensorimotor movement.

For example, if you're looking at a stapler and you move your eye, but there's also another signal coming from the magnocellular pathway that has smaller receptor fields and represents the patch movement. You're still looking at the same location, not moving your eye, but the object itself is changing or morphing in some way. It could also be color changing, and that's why you have patch movement. The last time we talked about it, we were opening that up to basically any change. Something is changing at that point, not necessarily moving. Moving would be a big part of it. In the stapler, it would be moving; the light on a traffic light isn't moving, but it's changing. So it's something changing there, but it's not related. To be clear, it's not that the patch is moving; it's the observed feature that the patch is observing. The feature is changing or moving. We're not talking about the patch moving. Do you agree with that? Yes, I agree, because we can't really observe the movement of the patch. We can just observe that, and that's not the important thing. The important thing is that the object itself is changing.

The features themselves are coming from a different pathway, the parvocellular pathway. These are not representing change; they're representing the actual features we're looking at.

To build models, the current theory is that we have two reference frames. We have an object reference frame. Both of these reference frames represent locations in physical 3D space, and they are learning: one is learning the features at these locations, and the other is learning movements at these locations. They're both synced and both use the sensorimotor movement to know where we are.

The object model and the behavior model are learned; this knowledge is embedded in the associative connections between features and locations, and between movement and locations. The model itself is in these associative connections. To be precise, although every location is observed at some physical location, the two reference frames represent different things: the one on the left represents the space of the object—the set of changing features—while the one on the right represents the set of static features. Although they are co-aligned in physical space, they do not represent physical space in the sense that they would re-anchor on different objects or behaviors. For someone new to this, it's important not to say these reference frames are physical space, or that they're in some egocentric space. One is in the space of the object, and the other is in the space of the extent of the changing features. It's subtle language, but it's important to use the correct terms.

This is the part where I may have some confusion, or maybe you're proposing something different. You did say "current theory"—I'm not proposing a change to the theory, just clarifying where I'm starting from. If you haven't spent a lot of time with grid cells, it's a hard concept. They anchor, and each time you're anchoring for a specific object or thing. For example, they're anchoring for the specific object, like the stapler, on the right. On the left, a similar set of grid cell modules is anchoring for movement or changes. That's basically all you can say about it.

I find it useful to dig deeper into the reference frames to make sure I have the same understanding. My understanding is that a reference frame is a kind of coordinate system—a metric coordinate system, as you usually refer to it, Jeff. It doesn't have an origin; it anchors to different objects but does not really have an origin. Objects are not related in space relative to each other. What I'm going to show next is not the right way of showing what anchoring is, because we don't really go in space and look at these objects like that. I understand that the representation of the location itself changes, or when it re-anchors to something, it's like it's initializing to that space, so there would be no conflict between those locations. But we're not really moving in space looking for these objects like that. This is the closest analogy I could make. In Monty, it's not implemented the way it is in the brain. In the brain, grid cell modules create a sparse distributed representation space. In Monty, we're using more traditional Cartesian coordinates, but we prepend a class modifier to put this in another space. When we talk about reference frames, we're really talking about two different implementations. We have to be careful, because I might be thinking about the brain's way, and you might be thinking about Monty's way, and they're not the same. I've been assured multiple times that it's fine that they're different, but we should keep track—they're really quite different. The way you show the picture corresponds more to how Monty does it, not how grid cells do it. These are not related in space like that. Once we re-anchor, it's not like we're moving to a different space; the representation itself, the grid cells, are randomized and initialized in a way that gives us a new space to represent the new object.

The important thing is that the location encodings aren't shared. A location SDR on the coffee mug is unique to that location on that mug, and we won't get that same location on a different object. The definition that a reference frame is a coordinate system without an origin isn't quite right. Grid cells don't really have an origin in the way they encode space, but usually people think of reference frames as having an origin, which is why we have an origin in Monty—because we're using Cartesian coordinates, and that requires defining an origin. We use the shorthand Jeff mentioned, prepending a class modifier so every object has its own location space, and coordinate (0,1,0) can exist in two objects.

In grid cell space, not only is there no origin, but the potential size of grid cell space is enormous—bigger than the size of the universe. Once you anchor, you can integrate around the set of points, but they're all SDRs, not X, Y, Z coordinates, and you'll never travel far enough to get to another object's part of the world. With Cartesian coordinates, you can say, "I'm going to put the reference frame for this object up here and another one over there," but it's not like that in brains. It's a very clever but totally different way of doing it. I'm not sure if that impacts what you're saying, but that's the difference.

As Viviane mentioned, the locations are unique to the object. Whatever location we define with respect to the anchor object is not shared by other objects. My understanding is that the movements are shared—the movement is shared. For example, "go right" or "go left" will move the location the same way; both will have to integrate properly.

I'm going to move into the more speculative parts now. This first part is the same—we already know this is how we learn features at locations. We get some patch features, move through the object's reference frame, and learn the associations that way. These are nice animations. How did you make them? I learned some transitions in PowerPoint; it's all done in PowerPoint. Is that occluded? Is there a stapler moving rather than a window moving? In that case, it's like a stapler object moving behind a window. I'm cropping it and just moving it. That's pretty impressive. I don't want to be too nitpicky, but the latest thinking we've had—at least, the latest thinking I've had—is that if we accept the idea of the center-surround receptive field, you wouldn't have an input that's all black. The bottom dot there is half white, half black. That makes a lot of sense from a biology point of view. It's not clear that you'd have any input at all when looking at a solid black area or solid anything. Features are always going to be some kind of non-uniformity. When we see a red circle, we're actually only learning the edges of the red circle; everything else is implied in between. It's a subtle difference, but worth mentioning. This is why we're always saying it's edges at locations or orientations. I've come to think of it as non-uniformity, because that's what center-surround receptive fields do. If you give them any kind of uniform pattern, they won't respond, but if you give them any kind of non-uniform pattern over their receptive field area, they will respond.

It's interesting to think about that in terms of 3D objects. With images, it's easy to picture—a circle, for example. It's more efficient not to try and learn all the points in the middle of the circle because it's all the same; you just learn the edges. But what does it mean when we have a 3D object where every surface is, in some sense, meaningful? You could have 3D corners and things like that, which might be more interesting. It's speculative, but if you follow the logic, a flat surface would be uniform, but a curved surface is not uniform—there's some information there. If I can detect that curvature, then it's non-uniform. For example, on the top surface of a table, you wouldn't need to learn all the points if you don't have to touch all the surfaces. Maybe even if the curvature is constant, that can also be approximated. I don't think we understand it well enough yet. It's hard to map edge detection as it happens with a 2D projection onto 3D features. I'm not saying it doesn't apply or make sense; it's just that we've talked about how we might get 3D features and where they might emerge, but I think the classic neuroscience view of what the thalamus is doing as it projects to V1 is still hard to map onto where 3D features come from, because that's normally not discussed. We have this 2D field—this is the first time I've heard us discussing 3D features, or at least I forgot.

When we were talking about motion parallax, binocular vision, and things like that, we found some evidence that at least in mice, motion parallax seems to lead to very early representations of depth. Representation of depth is not necessarily the same as 3D features. A feature is a patch—a sensorimotor patch. The way I've been thinking about it is, you take a sensorimotor patch, and if it's got any kind of center-surround receptor field—so that could be a part of a flat area of the retina or the flat area of your skin—within that patch, if there's any non-uniformity, you'll have an input. You can just follow the logic from that. Obviously, there are no 3D features on the retina, but we could probably go down a rabbit hole on this for hours. I don't want to distract; I keep coming back to this idea that you could just think about center-surround receptive fields. If that's true, it sort of answers all these questions. You could just keep going back to what can be represented with the center-surround receptive field—what would make it respond and what would not. I probably said enough on this. We haven't let Ramy get to his main point yet.

Okay. As I said, we just learned the model this way. But my question is, do we also want to learn? My thinking is that we should not want to learn multiple models for that object at different behaviors. We should not want to build associative connections of the features as they move; at every behavioral state, we are not building a new model. My understanding is that we're building just one model of the stapler and then applying behaviors to it, but we're not learning multiple models of different behaviors for the same object. We didn't actually reach a conclusion on this, Ramy. We certainly don't want to learn a new morphological model for every position of the staple top. We can't; we don't have the time to do that. But on the other hand, we can infer the object in its different positions. Obviously, if an object has two main positions—maybe it's always open or always closed and then moves between them—then you might say, "Yeah, I'll learn the open one and the closed one, and I learned that they move between them," but it seems impractical to try to learn everything in between. This is very much what we were just talking about in the morphological model. We said, if you've got edges where non-uniformity is occurring, you imply that the surface of an object continues even though you're not sensing it. In between here, it could be the same thing. I have a set of movement vectors; I'm representing the behavior of the top of the staple moving. I might be able to generate an image of what the staple should look like at any point along that trajectory, even though I haven't learned it. I can fill it in and say, "If it's moved this much, this is what it's going to look like," not that I've learned the morphological model of that. We haven't figured this all out. If we understand, though, these are reasonable assumptions.

I think in general the feeling was it may be somewhere in between, where we may need to learn some kind of important key frames, but we don't want to densely sample that space. We can maybe talk more about path integration later if that comes up, because I think that is also relevant. The reason I'm bringing this up is that I think there are a lot of parallels between learning this and learning the behavior reference frame. Whatever rules we come up with here are going to transfer to the behavior reference frame, and I'll discuss what I mean in a second.

I'm going to assume here that we don't want to learn this because it would require a lot of associative connections—learning a new model for every behavior, a new behavior for every model, and then you have a lot of models. It's just impractical to do that.

The other side of this is learning movements at locations. What I want to discuss is that we are getting patch movements. Now, it's a feature change, and we're learning it at a location.

But we're being anchored here to a behavior, and we want to anchor to this behavior regardless of the object we're seeing.

I think we agree that we probably don't want to anchor to a behavior and an object at the same time, because then we get into a combinatorial problem: every behavior has different objects, and another behavior will have a different object. We cannot anchor to both at the same time. We have to anchor to a behavior, but couldn't we have two separate reference frames, and they can anchor at different times or at the same time? Whether one of them anchors or the other shouldn't affect each other, at least in the current setup of the idea. But why would we want a reference frame that anchors to a behavior for a specific object? Are we anchoring? No, we don't. Think about it this way: you have the world of behaviors, which is how things change relative to each other, and you have a world of objects, where features are static relative to each other. When you first learn a behavior, it's always going to be associated with a particular object. I haven't been able to think of learning a behavior independent of an object. So you're going to learn someplace, and we can associate the whole behavior with an object. But the behavior has its own reference frame, and the object has its own reference frame. We can associate the behavior ID with a location on the object, but now I can apply that behavior to other objects, right? The behavior definition is not tied to the particular object, although I can associate it with a particular object. For example, state booths have hinges, refrigerators have hinges, and doors have hinges, but water bottles don't have hinges.

So, yes, this makes sense. Would we be able to learn disassociation from the interaction between the behavior reference frame and the object reference frame?

What we represent in the behavior reference frame is an abstract behavior state, but it's associatively linked to the object in the object reference frame. Instead of representing relative locations in the behavior reference frame, it must be associatively linked, because when we see the staple, we know it can open and close. I associate that behavior with the staple. So there had to be an associative link between the two at some point.

The solution we proposed so far doesn't really talk about the associative link. It's basically just to be able to recognize behaviors and recognize objects and recognize them independent of each other. Like Jeff said, they're basically two separate worlds. If you sense a specific movement, you can recognize that pattern of movements. If you sense a particular object, you can recognize that pattern of features, but they don't inform each other at all right now in the current proposal. Like Jeff said, we need to be able to do it, but as far as I remember, we haven't proposed specifically where that would happen—if we have an association between Object and Behavior ID in Layer 2/3, or if we have an association between the two reference frames. We haven't discussed it, but it's obvious it has to happen.

It seems obvious to me that this has to happen. Another possibility we haven't discussed is that a behavior is typically smaller than the object, and perhaps the compositional mechanism could be used for that. You could learn that an object has, in some sense, a child behavioral object that is applied at one location and orientation. That's another way to do it. You could learn between two regions or two columns—actually, two columns and two hierarchical regions. The behavior is a child of the parent mythology of it.

I think that's interesting. It's like the sub-object behaviors we talked about before, though I'm forgetting the exact example we had where it breaks off. But keep going on this idea. The problem is, first of all, a parent object can have multiple behaviors. We talked about a stapler having a deflection plate behavior, the hinge top behavior, and other behaviors like adding staples.

Each of those behaviors exists at some location, scale, and orientation relative to the parent. That's exactly the language we use for compositional objects. It seems logical to think the same mechanism for compositional objects, like the logo on the cup, would work for the behavior of the hinge on the stapler. That's a pretty interesting idea. I completely agree. There's a lot in common in how we learn the positionality and object behaviors. I think it's the same learning mechanism. I tried to talk about it in the write-up, but I don't think I did a good job. I'm just trying to make comparisons between learning features at locations and learning movements at locations. In features at locations, we're anchoring to an object and want to represent that object regardless of what behavior it's at.

We don't want to learn multiple models where every model would have a separate behavior. In learning movements at locations, we're learning a behavior, anchoring to a behavior, and want to represent that regardless of what model is exhibiting that behavior. Because we are anchoring to a behavior, we don't really care what the model is. This is part of recognizing the behavior. If we want to recognize the behavior regardless of the object exhibiting it, it's probably reasonable to assume that how we get there is a different story. How we learn that is a different story, but it also hints at how I'm thinking about these locations. I think these locations are somehow linked to physical locations on different objects with different morphologies. A laptop top is different from a stapler top. In my thinking, a location in the behavior reference frame needs to be linked to different physical 3D locations that describe different objects, so it could apply these movements on them at the same time. By saying that, it also means that the locations in the behavior reference frame are not necessarily physical 3D locations, because they need to be associated with different objects that have different morphologies in physical space.

But aren't they still locally spatial? They might anchor at different times to different parts of an object, but if a sensorimotor moves in the behavioral and in the morphology space, once it's anchored, it would still be the same—it would still be moving in 3D space. I think it starts this way. When we first observe a behavior, like a stapler opening or closing, we learn it on a stapler, but as the behavior becomes more abstract, we move toward a definition of a location in the behavior reference frame that is not tied to the movement, but to moving along the behavior trajectory. What I'm imagining is something like this: we're anchored to some behavior, like a scale behavior, and the locations—the red dots—are moving along the trajectory of the behavior. We didn't learn it this way; it became this way. The definition of the location has morphed so it can represent different objects at the same time.

Are you saying that you think the behaviors aren't learned initially in the behavioral space, but learned in the object's morphology space, and then later it gets learned or transferred? I'm saying it is originally learned tied to a stapler's morphology in its own reference frame, but the definition of the location in this behavior reference frame will change over time to represent multiple objects. The location—no, wait, let's be careful there. The behavior, the space of the behavior, I don't think it's ever going to change.

You can associate the behavior at a particular location on a particular object, but the behavior is defined by its grid cell space, just as an object is defined by its space. Once I know the object, all the points the object might occupy are defined. Up to now, we've had this ironclad rule that, at least in grid cells, once you've anchored, anchoring is synonymous with recognizing the thing. Once I've anchored in the object's morphology space, I know what the object is, and once I've anchored in the behavior space, I know what the behavior is. But those locations wouldn't change—that's how we've been thinking about it. If you're suggesting they do change, it's not that the location changes, but the meaning of the location. What I'm suggesting is that at the beginning of learning, maybe with a hinge behavior, we are learning that behavior specific to the stapler. This is how we know how to identify this behavior—it's on a stapler. As we start abstracting it more and more, the location here cannot be represented in physical space in the morphology of the stapler.

Let me stop you there, Ramy. The way we've been viewing it is that the definition of an object is static features at positions or poses in some reference frame, and the definition of a behavior is changing features at poses in a reference frame. From the very beginning, if I have a changing feature, it's only going to get stored in the behavioral object, and if I have a static feature, it can only get stored in the morphology. They're separated from birth. If it's changing, it's going into the behavioral space; if it's not changing, it'll go into the object's morphology space. If I understood what you said, that's counter to what you were saying.

No, I agree. I don't think it's counter. What I'm saying is that the behavior reference frame locations need to be abstract enough to represent multiple things, but they are from the start. They're the locations in the behavior space. They only make sense in the behavior space; they have no actual correlation or tie to the locations in the morphology space. They could be associatively bound, but it's not like they are directly connected. As you say, it's a separate definition. From the very beginning, the behavioral object or sequence is its own set of SDR locations, and it never changes. From what you were saying earlier, Rob, it feels like in that behavioral space, it's still important that it's a space. There's a temporal dimension as a complication, but you can still move through that space with your sensorimotor system, and that affects what you predict. I agree, we need to be able to scale it up or even deform it to apply it to different objects, but that feels more about where you bind and maybe how large the behavioral space is, rather than it becoming more abstract.

Niels said all that gets answered if we take the idea that behaviors are child objects of larger morphology objects. If they work the same way as the logo on the coffee cup, but it's the hinging behavior on the stapler, then for the logo on the coffee cup, we have to do scaling, orientation changes, and morphing. We came up with the answer to that—it was hard, but we did it. I think that could apply here too. I can have a behavior, say hinging, and when I apply it to a particular object, I'll do it the same way I applied the logo to the cup. I can do it on a location basis and learn those associations as needed. Therefore, the behavioral model can be applied to a location on a parent morphology model.

I like that idea. We've already talked about how behaviors can be recognized at different scales, orientations, and locations, but the one thing I was still wondering about was deformation. If you imagine the stapler top is curved and moved up and down, you can still recognize that hinge behavior, but it's in different locations. Niels reminded me the other day that in a hierarchy, we already have a solution for object deformation. We can learn the bent logo by assigning the logo location by location to the cup and assigning it at different orientations and locations. That would also be a solution for assigning a bent behavior to an object quality.

That's exactly what I was saying. I was just putting it in my own words. Maybe you knew that already. That was a thread on Slack, so you might not have seen it in the last few days.

I'm still a little confused about the locations and the behavior reference frame, so let me ask it a different way.

These locations here, even though they're completely different SDRs because they're in different reference frames and are relative locations, even though we don't store any features, orientations, or whatever here, the relative locations between them still in some way define the shape of a stapler. No, they don't. No, they don't. Something we've not addressed is the assumption that movements could be scaled by different amounts in these two reference frames. Actually, I don't think you need that. When you adapt to a new object, you would scale it. By the way, let's go back to what Ramy just said. Ramy, you said that even though the behavioral model does not represent static features, it still defines the shape of the stapler. I think that's what you said. Yes. I disagree with that. The behavioral model wouldn't represent anything that's not moving. If there's a part of the stapler that's not moving, it's not going to say anything about it. You can have a behavioral model that's just a deflection plate. That's not going to tell you what a stapler is or what a staple shape is. It does nothing about that. It just says within this local area, a set of changes can occur, and I can learn that set of changes over time.

It really is a subset of the whole stapler, just like the logo on the coffee cup is a subset of the coffee cup. It's not the whole coffee cup. Is that right? I agree. The point is it's still a subset of the locations on the stapler. At any point, it might be applied to a subset of locations of a staple. It could be applied to your laptop; it could be applied anywhere there's a hinge.

That's because these reference frames are synced, and the relative locations in both the behavior and the object model aren't the same? No, not still. That's how it's learned when you're learning it. If you're looking at a physical object and it exhibits behaviors, sure. But again, think about the stapler. There's a reference frame for the deflection plate behavior, and there's a reference frame for the hinge behavior. Those are completely different reference frames. At any point in time on a particular staple, I could apply the reference frame for the deflection plate at one point and the reference frame for the hinge at another point. Now we have three different reference frames. They're really unrelated to each other, only in intent, in a particular object.

When you recognize them, you can also recognize them in different skill sets. You might learn the hinge on the stapler but then recognize it in a totally different orientation and scale on a different object, like on a door or on a laptop. When you recognize a behavior on a new object, that reference frame—how you move through it—might look totally different than how you move through the object's reference frame because you're actually recognizing it at a different orientation, location, and scale.

It's still not clear to me how the relative location was similar to the stapler, but we can recognize it on a different object. I know we're still storing movements, but these movements are defined at specific relative locations. Maybe if it helps, with a stapler versus a laptop: when you learn the stapler, you moved five centimeters or whatever from the hinge point to the tip, and that stored that relative movement both in the morphological and in the behavioral model. The recognition of that behavior is this kind of movement of this hinge-type thing. When you then see the laptop open and you go to the tip of the laptop, that's like moving most quickly to the end. That's where the hypothesizing in the scale of the behavioral model would basically say, "Okay, I think the behavioral model is actually larger because it's mapping onto this larger object." The location, which is the tip of the behavioral model, is going to be bound to the tip of the laptop lid, and the corner, which is now separated by a large distance. This ties into what we discussed the other week, where Viviane pointed out we probably want to be able to transform the movements separately for the behavioral and the object models because they may have different rotations and different scales. I think what you're saying makes sense.

It's necessary that if we haven't seen a hinge behavior before and we learn it on a stapler, and we're going to recognize it on an object, that hinge behavior needs to be applied on a morphology that's close so we can still anchor to the same behavior. But I think that by anchoring both the stapler and the laptop to the same behavior, this is going to change—it doesn't have to be close. Remember, Rami, at the last retreat, I brought up the example of having a chair where there's a little piece in the back leg that had a hinge on it. It was a totally made-up idea.

The chair is nothing like a stapler, and it's only a little part of the chair. You have to think of these as completely separate objects. The behavior object and morphology objects are each a set, and they can be mixed and matched in different ways, at different locations, different scales, and different orientations. It might be an issue of introspection and trying to think of examples where you're applying behaviors to morphologies.

I get your point. I was first thinking like that too, that we're still defining those changes at locations, and those locations tell us a bit about the shape of the object. But your picture is accurate: the behavior model is just these red arrows at these locations. If I look only at the behavior model, I wouldn't be able to say that this is a stapler. But the fact that they are at these relative locations means I will only recognize that behavior if I see changes at those relative locations, and I won't see them on objects that don't have a solid straight thing that can move like that. If I am looking at a ball, I will never recognize the hinge behavior because I can't see these changes on a ball, unless it literally splits in half and develops a straight edge that could then map onto it. When it's a solid ball, I would see these changes relative to each other, how they were stored in the behavior model. If I'm not seeing them, I'm not recognizing them. But it's independent of the actual object itself; it's just what's physically possible to observe.

I think I see your point.

Do you want to talk a little bit more about the abstract stuff? We're trying to at least explain how we're currently thinking about it, but I don't want that to get in the way of you talking about your ideas. It might build on that in some way or show us a problem with what we're thinking. Before this conversation, this is what I was going to propose: we anchor to a specific behavior, and the movement becomes abstract enough that the locations here do not represent relative locations on specific objects, but rather a location in the trajectory of the behavior. The movement would basically be moving it into an open state or a closed state. I can explain more about what I mean here, but is that a space where a sensorimotor can still move through to make predictions?

The sensorimotor will still move. Let me finish that. These are the end of the slides—just a few, four slides. I'm going to contrast behavior reference frame and object reference frame as I see it. The object reference frame on the right anchors to an object, while the behavior reference frame anchors to a behavior. An object reference frame will anchor to an object regardless of what behaviors the object is exhibiting—different colors, states, or scale. If we assume scale is a behavior, this is a desirable feature: we want to factorize behavior from the object representation so we don't build too many associative connections in the object reference frame. For recognition, we want to generalize so we can recognize the object in different conditions. If I take my stapler and put it under the sun or in a dark room, I still want to recognize it. This is the kind of variability we want to recognize.

For behavior reference frames, it's a similar idea. We want to anchor to a behavior regardless of what object is exhibiting that behavior. We need the idea of abstract locations that can represent multiple different objects. They can associate with multiple different objects in physical 3D space, but the location itself in the behavior reference frame is more abstract. As we move through, as we change the location in the behavior reference frame, we move along the trajectory of the behavior.

On the right is the object reference frame; on the left is the behavior reference frame. The behavior can change the object. In the middle is a superposition of both, or some sort of conjunction code. To represent the object at a specific behavior, we need both locations: the location from the object reference frame and the location in the behavior reference frame. The behavior does not represent the object in physical space; it just represents the behavior, but it can be applied to any object. The object locations can be applied to the same object but in multiple different behaviors. You can change the behavior, but changing the behavior doesn't necessarily change your sensorimotor location. For example, you can change the sensorimotor location without changing the behavior. The behavior could be an open stapler, and you can change the sensorimotor location on it. You're still detecting it's an open stapler, but you're changing your location on the open stapler, or a closed stapler. The sensorimotor movement only controls location changes in the object reference frame. The movement of the object controls the location in the behavior reference frame. You can have movement here, but you don't need to have movement there. The superposition or associative connections between both gives you the ability to predict where on the object, given the object and the behavior. We still need to move in the behavior reference frame too, if we're moving the sensorimotor, so we know where in the behavior model we are and what changes to predict at that new location. If the behavior is not changing—if it's an open stapler and you're just moving across the object with the sensorimotor—you don't need to know anything about the behavior except its location in the trajectory of the behavior. I know it's an open stapler. I think what Viviane was saying is that while you're observing the behavior, you have to observe the locations that are changing, and there are multiple ones.

Your sensorimotor pads can only observe one at a time. You need to be able to, if you're observing a change, know where you are in the space of the behavioral model and therefore your location in the behavior model. That location, or where you are in the trajectory space, is a function of your location in physical space and your location in time, because the stapler will have a different behavior toward the end of where it closes than at the very start. That's because you're at a different location.

When the sensorimotor movement goes into both reference frames, you move through both in synchrony using the sensorimotor movement. The question is about the picture on the first slide, where the sensorimotor movement goes into both reference frames and you move through both in synchrony—this one or the very first slide. The sensorimotor movement at the bottom goes into both the object and the behavior reference frame. The sensorimotor moves in both reference frames, because if I have the opening stapler and I move from the top to the bottom, I need to make predictions about the changes I'm going to see next.

This goes back to my assumption that the location in the reference frame becomes abstract. The location now represents the whole behavior state. Now you have a morphology of the object and a location in the behavior reference frame that represents what state it's in. As you move, you can predict what the rest of the object will be like, because you're learning that this location is not a physical location—it's a location on the trajectory of the behavior. You're changing from changes at physical locations to pulling over all the changes to define an object state and then moving through that state space.

Why are you just storing information about local changes? How do you still make predictions about local changes? If you get rid of that, you can still make predictions. The way I described it here, this state is just a behavior state. We have the model, and both describe what the actual state of the object is.

It's an open stapler or a closed stapler. You can make predictions using the associative connections between the behavior reference frame and the object reference frame.

It's like a superposition of these two codes that gives you a full view of what's happening.

For example, with the stapler opening, imagine as it's opening, there's something occluding your view in between. Now you're going to move your eye to where it's no longer occluded, and you'll be able to predict what that movement will look like at that point. That's a location because you're predicting a state—a location in the behavioral model—but that location is a function of your location in space, which is where you path integrate with your sensorimotor movement. You learn that a feature changes at a particular location.

How would you predict something without that? I guess you wouldn't need to. When we learn an object model, we're learning features at locations, and we can predict what's happening here because we have an object model. When we have a behavior model, we can learn what's going to happen somewhere else because we know how that behavior affects the object. But how do you know you've moved to that location in behavioral state? How do I know to expect a particular point in behavioral space? I could be anywhere in behavioral space.

No, you're still tracking your behavior. This is where it's occluded, so I have to make a prediction at a future point or a new location. What exactly is occluded? The object is occluded—the movement of the stapler head. If the movement of the stapler head is occluded and you have not anchored to the right behavior, then you cannot predict what it's going to look like. I'm not saying it's occluded the whole time. This was an example Jeff brought up previously: a stapler is opening, and at 45 degrees, it starts going behind a little screen, then comes out again.

When it comes out again, you don't predict that the stapler head is going in the exact same feature movement that was seen at this point in time and space. That's not what you predict at the other end. You predict a new one, which is a function of where it's moved ahead in behavioral space. You're anchored to a behavior. If I'm looking at this stapler and I occlude part of it and still move it, because of the movement I've seen from the part that's not occluded, I can anchor to the right behavior and predict where I am in the behavior. That still gives me the full model of what's happening behind the occlusion. You can still predict all the points behind it because this represents the behavior state rather than the behavior at the locations—the movement at every location. Maybe I'm not describing it very well. How do you even still represent behaviors in this solution? Where do you store the changes at locations to recognize that there's a hinge behavior going on? Where do I store the changes at relative locations that we have in the object's reference frame?

It's the same theory we already have. I'm still thinking that the location does not become the relative. These locations will morph and change to become locations of states. We can move between these states rather than a location in physical 3D. They don't move the same way that when the sensorimotor moves, they change. It's not a location like that anymore. It's a location in the trajectory of the behavior, but it represents the state. I think we're a bit confused with what the states are now doing in the behavior reference frame, because those are not changes anymore. When you say it is in the closed state and it's not moving and you're moving on it, that's not in the behavior model because there's no change happening. They still store changes at locations. For example, if you've only seen the behavior on the stapler and the laptop, then the location is going to be the full stapler top and the laptop. This point becomes the whole top of the object, and then you're storing a movement at this location, which represents the top of the stapler and the top of the laptop. As you move in the behavior reference frame, it represents the change in the location. It is just a change in the state. Basically, it becomes this change in the state.

Ramy, I wonder if all this confusion is coming about because you have a different idea of what we mean by locations. Sometimes you say physical locations, but nothing in the behavioral model or the morphology model involves physical locations. Those locations in those models are all abstract, just relative to each other. Of course, an object will appear somewhere physically in the world, but I'm trying to figure out what's confusing you or what issue you're trying to get at, because I don't see that issue. I'm wondering if you have a different idea of what we mean by the space of the morphology. It's not a physical space. A stapler could be big or small; it's a set of locations relative to each other, and that's all it is in the morphology model and the behavior model. After that, they're nearly identical. The same process is going on everywhere. I think there's a more fundamental misconception or a difference of opinion here, where you think the stapler space is more real than the behavioral space. They're both equally as real or non-real, both as abstract or not abstract, however we want to call them. Is it possible you're thinking along those lines?

Yes, I'm thinking that they are different. Maybe this is what we need to address. As I view it, they're not different at all. They're virtually identical. In one space you're storing the changes, in the other space you're storing static features—changing features and static features—but after that, they're identical types of spaces.

The fact that you keep saying "physical space," I would rarely use that term. I have the space of the object, which is self-referential, relative to the object. When I define a model of an object, it's not physical space; it's stapler space, and it's all relative to the stapler, but nothing else physical about it. The same thing with the behavioral model—it's relative to the space where things are changing, and that's what I'm storing in that model.

I think there might be a more fundamental problem here. Maybe that is right. You probably pointed at the problem. My confusion point is that, first of all, this is a really hard concept. It took me months to understand grid cells. I read paper after paper, people tried to explain it to me, and it took me months until I got it. Maybe I'm slow, but it's hard, and the concepts are really hard. Maybe this tells us we need a better way of explaining some of this stuff.

I don't know. It could be an education issue. I'm curious because these were really nice slides and a nice discussion. I agree with that. It was helpful for me to understand our current approach better. I'm curious if you go away after today and think about what we were arguing—where you can scale and location by location bind behaviors—but that, at least in our heads right now, seems sufficient to do a lot of this. If you identify issues with that, it would be helpful to bring in concretely how you see that helping with this. That's also part of the challenge—right now, when you describe this, we're more seeing it as taking away, like being able to integrate with the sensorimotor moving and so on. It's not clear what it's adding, but maybe there is something we're missing.

One other thing this highlights, or at least makes me think, is that in the behavior model, there is actually another space besides the reference frame of locations. We have the temporal dimension as well, which is the state space. If I understand you, this is maybe a bit like the state space you're describing, where through time we can move through different states of the object. That's an open question that came up before—if we can actually path integrate through state space, like if we have a joystick, we know how to go from wherever to wherever in that state space, or if those are discrete sequences and we have to learn a bunch of different sequences to go through. A second related question—I forget if it was you, Scott, or Jose—when we were talking about the diagram last time and the temporal input going to the behavior model, is whether there's also some temporal dimension to the object model, where we can learn certain key states of the object's morphology, like the closed stapler and the open stapler.

Just that, not a topic for right now, but there is actually a second space in the behavior model, which is the temporal state space. I agree it's under-discussed in the solution so far. Do we have key frames in time as well? How do we represent that without having another explosion in how many representations we need?

One or—oh, sorry Jeff, you're on mute. I'm not too worried about that. I think it's worth exploring, but the behavioral state is pretty sparse compared to the object space and morphology space. It's only where things are changing, and they don't change forever. The same location doesn't usually change twice, so I'm not sure about it being too dense. I am concerned about how we learn it. At our retreat, I kept bringing this up: how can we learn? I can't imagine the final behavioral space being populated properly, and I don't see how a single sensorimotor patch could learn it. A single sensorimotor patch can't observe all of it at once. 

Let me try to summarize several things I've heard here today. First, really smart people like Roi King—perhaps, maybe I don't want to prejudge this—but maybe you end up with an incorrect idea about what the space is, so we need to address that from an education point of view. Second, I think today for the first time we discussed representing behaviors as children of morphology objects. I don't recall talking about that before, and I think that's a really great idea. We should explore that, and it addresses a lot of the issues we're discussing, just as Neil summarized. Third, we still have the issue of filling in. We can't learn every edge of a table or every face of a cup, and we can't fill in and learn every single point in a behavioral space. We have to be able to fill in between, to take the knowledge we do have and fill in the consistencies in between—an old idea, but I think we never fully addressed it.

Those are my takeaways so far today. That's just my personal summary.

Two other things came up: acknowledging again that we might need to learn multiple morphology models—at least a constrained set—but what is that number? How is it a function of the complexity of an object? I guess that ties in with the filling-in idea you were talking about. I think it does. In the temporal dimension, I had a brief point about path integration, unless there was more in your presentation, Ramy.

No, that's all. Thank you all for the feedback. It was really great. Thank you. That was really good. Those are great images, by the way—really nice presentation. Easy to think about that.

It's just a silly introspection thing. We use introspection a lot, and the other day when I was bouldering, you have this concept of reading the climb. When you're standing on the ground looking at all these handholds, it's a mess, but rather than just jumping on and trying to climb, you try to picture yourself moving through space and actually doing the climb mentally. I thought it was an interesting example of mentally simulating a behavior because it's really hard to do. In general, you can only do it over short horizons, and if you're going to do it at all, you have to anchor after a certain horizon and then go from there.

It got me thinking that, and we've talked about this before, but a kind of resolution to the question of path integration versus not path integration is that we probably can path integrate in a lot of behaviors, but only over very short horizons. I think this fits well with the biology of grid cells. The more grid cells have been understood, the more it's clear they aren't a very good metric of space over large distances—they quickly become noisy and distorted. But they're quite good as local metrics of space. As long as you don't expect to be able to, in the case of sensorimotor movement, path integrate very far with accuracy—and maybe also in the case of behavioral movement, move through a behavioral trajectory very far before you reestablish where you are—then you can do some path integration, but it's local. It might make it easier to learn, like we've talked about: in a local space, it's easier to densely sample and almost learn it as a series of sequences.

I don't know if this is an argument for the temporal aspect being path integratable as well. I think so, but just over short horizons. I don't know what it means to be path integrable in time. I don't know what that means.

In time, it's taking different trajectories—maybe path integrable in state space. If you imagine an object can be in different states, like a stapler can be open, closed, or in between, then knowing how to get from one state to another—you observe how it opens, but maybe it can also move in a curved path to the closed state. Being able to take a new path to go from one state to another—it's not really through time, it's through state space.

Because, as you say, time is linear. Once you're on the trajectory of time, you can't go back or move off time. Or go outside of time—time travel. No, I think you're right. Maybe it's more like behavioral space. Maybe a better example is a stick shift in a car. You know the different states it can be in and, from every gear, how to get into every other gear, even if you've never done a specific transition—like shifting from gear one to gear six. But you still know how you would get there. You might have to think about it a bit more.

I think it's second nature going from one to two to three. You probably have to think about it, but it's not like you have to have experienced it to recognize the connection between them. You can infer it. I have one question for you, Neil. When you’re bouldering, do they have a harness on you? No, but that's because it's very low down. Should we allow this behavior? I'm not sure this would be acceptable in the Thousand Brains Project. We don't want to have injuries. There are more hardcore climbers. I can promise you, I'm more of a scaredy cat than Ben and Lucas; I'm not doing anything too crazy.

One funny side note: when you gave that example of scoping out the route, usually you actually do the physical movement while you're looking at it. Often, you see people imagining the moves, but they actually do it on the ground, just moving their hands as they're planning to move them. Maybe that's part of the process as well.

I was thinking, in general, we're going to have to get better at dealing with noise as we move Monty into the real world. Even just path integration in space for morphology models—we can't assume that's noise-free in the long term. Is that an issue if we can't do it? Over longer distances, maybe, but I think with coarser models, for example, that would deal with things like large decoding errors. I've used this example many times: when you're walking in a room at night and it's dark, very quickly as I leave my bed, I can sense the uncertainty increasing. Suddenly, I have no idea where I am. But as soon as you catch a single thing, like the edge of a door, you immediately know where you are again. You can observe that it's a very inaccurate, noisy process, even over medium or short distances. But I think it's not a problem as long as you're able to re-anchor with a single observation.

I think it's something we might want to include. I don't know if you've heard, Jeff, but Rami's working on some cool stuff with re-anchoring and re-sampling hypotheses at the moment. That sounds great. I think that'll be a nice addition to Monty.

Today's discussion reinforced for me, even though Rami is poking holes in it, the validity of the idea that these two models are really parallel. Now we've introduced the idea that they might be parallel in hierarchy as well, but it's not a hierarchy of behavioral state—it's a hierarchy of a behavioral state on a morphology, which addresses some of the things Rami was trying to get at, like how you apply this to a particular morphology model. That also fits with our earlier discussions about how behaviors would be communicated in the hierarchy. One of the issues I had with hierarchical behaviors is that the behavior model ID isn't really a change anymore; it's like a static feature. It would make more sense to assign that to the morphology model in the higher-level region. I really like the idea of using that as a solution for assigning behaviors and being able to deform them and assign them to different locations on an object. It's interesting because we just said, "Let's apply what we know—there must be a hierarchy of behaviors, and they're going to work the same way as the hierarchy of morphology models," but it was a slight twist on that. I think that's actually quite a good advance for today. That's how it feels to me. However we got there, thank you, Ramy. I think it's great progress. You can be our foil, out there poking holes—rubber duck. I thought it was also a really nice explanation of where we are so far, especially the first slides, and a really nice visualization. I was thinking this might be a good first video to release if we share this idea, at least the first part of recapping. It was helpful to me to go through the basics of the reference frames and the current theory we have. I think we still need to go through the basics again, because there seems to be some confusion about it.

I'm not sure what the best way of doing that is.

Other than I've said it multiple times, these are very interesting and challenging conceptual things to absorb. That's a personal observation about myself, and we should expect that lots of people are going to have trouble understanding this stuff right from the beginning. Maybe one thing I have trouble with for sure—I'll put my hand up. I don't know if someone should do a review of grid cells and anchoring and so on; that might be useful at some point. I'm also wondering how much we've talked about how we think about scale. I know we've had brainstorming sessions on that back in 2022 and 2023 when it was just Viviane and you, but I don't know how much we've talked about that recently. That might be something we're taking for granted. I think we have a reasonable idea—maybe not the biology, which is still a bit hand-wavy or unclear—but at least in terms of how we would implement it today in Monty, I think we have a reasonable idea. It's not that different from rotations. I would still be interested in going over that again because I think there was only one in-depth presentation that Jeff gave about it, about how it might work in biology, and I was sick that day. Maybe that would be a good topic for another time. It's a pretty simple idea.

I say it's simple because it may not be simple to convey.

I'm not prepared to do that today, so maybe we could do it another time. It's also the kind of 10 o'clock. It's fine. If we wanted to do a presentation on that, I'd be happy to talk about how I think it could make sense to integrate into Monty, if you wanted to discuss the biology or something. Just one last note: if we review grid cells and all that, maybe also include a couple of slides towards the end about our current theory for how to apply this to an abstract space, or what locations in these spaces mean. I know we don't have an actual theory for that yet. I had an interesting idea about abstract spaces—it's really just a half-baked idea, maybe even less than half.

It starts with a definition of what is an abstract space. It's when you are building a model of something you can't directly observe. In that regard, V1 or S1 would never build an abstract concept, but V2 could, because V1 is getting an object from V1 in terms of the hierarchy. We're talking about compositional structure. V2 is building a model based in a physical reference frame that's updated with physical movement. I'm updating the reference frame with physical movement, but the things I'm assigning to those locations—some of that information is coming from V1, which is no longer a direct observation. It's an abstraction of what V1 thinks it's learned. V1 says, "I think I know what these features together mean, and maybe they represent a letter E," but then V2 is getting this thing and says, "I have no idea what V1 is sending me. It's just some SDR." But I'm going to build models based on it. A while back, I said I felt that abstraction begins very early on in the cortex, and I think this is what I thought of at that time. I couldn't remember it then. It needs a lot of fleshing out, but it's marrying two things. One is we might have to have a space or morphology of a space that is some physical stuff. If I'm learning mathematics or physics, I might draw a picture of an atom or a timeline. So I have to have a—do all abstract models have to be based on some sort of physical model? Let's say they are. But the population of the model is stuff that has nothing to do with that; it's derived early from somewhere else. It's no longer an observation of the world.

The concept of—I don't know. I don't want to speculate further. Let me just let that sit for a bit. Abstract models are basically any model where the features are not directly observable in the world and had to come from something else. There needs to be a lot more elaboration on this idea, but it seems like an interesting way of thinking about abstraction. In some sense, it comes down to what is the meaning we assign to locations in that reference frame. In V1, I'm observing something. There's nothing else being assigned to a location in V1 other than something coming from the retina. In S1, there's nothing other than something coming from your skin. There could be some exceptions, but let's not go there. As soon as I'm going up the hierarchy and doing compositional structure, now a column is building models of where the features are unknown or already abstracted from the world. They may not be real or certainly can't be observable directly. I can't observe directly. I don't have a sensorimotor system that directly recognizes Ramy. I don't have a Ramy feature detector. You are an abstraction based on a lot of other features and things I'm learning. So anyway, that's just a start on that path. We immediately jump to mathematics and physics and quantum physics, how we understand these things. But I think we can start with the basics of describing it and work our way up to that understanding. That's just food for thought.

I like that framing because I feel like usually when we talk about abstract space, we actually talk about location space—whether we can have non-3D or higher-dimensional or different kinds of space. Defining it by the features is an interesting way of looking at it. Just to resurface it again, I'm currently sold on the idea that everything is represented in 3D or less space, but one idea that I think you presented, Jeff, a while ago was that potentially space could be more generally framed as each mini column learning a one-dimensional movement vector. Then, by combining them, you can learn different types of spaces and how to move through them. It's a great idea, although I'm not sure it has any validity.

It felt like the idea just presented was going the opposite direction. Like you said, it's like saying space itself is not abstract, and I'm not willing to say that. I also don't know if they need to be incompatible. I feel like abstraction could partly be about the input features and partly about space. Let's think about the example you use all the time—a family relationship. One way to think about it is that the way we've presented it is that there is some space that's not physical in any sense, but it's representing the relationships between people. The other way is saying perhaps it really is just you're drawing a picture of a tree, and that's what we're looking at, and that's how the model works. Then somehow the abstraction comes from the features and how they are composed in that space. It's a fuzzy idea, but it touches on what I think Vin was just saying—that maybe space really is always two or three-dimensional, and yet the abstraction comes not from the change of the space itself, but in the features and how they're organized hierarchically. 

Anyway, it is past bedtime for you guys, and I have to get going. I think it might be a good time to stop. Is that all right? 

Yeah, sounds good.